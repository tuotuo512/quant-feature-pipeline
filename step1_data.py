#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ä¸‹è½½è„šæœ¬ - æ”¯æŒå¤šäº¤æ˜“æ‰€ã€å¤šå¸‚åœºç±»å‹
ç”¨é€”: ä¸‹è½½åŸå§‹Kçº¿æ•°æ®ï¼ˆè®­ç»ƒ+å®ç›˜å…±ç”¨ï¼‰
é…ç½®: congfigs/step1_data download.yaml + main_config.yaml
"""
from __future__ import annotations

import os
import sys
from datetime import datetime, timedelta
import time
from typing import Optional

try:
    import pandas as pd
    import ccxt
except ImportError as e:
    print(f"âŒ å¯¼å…¥ä¾èµ–å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install pandas ccxt")
    sys.exit(1)

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ç»Ÿä¸€ä»£ç†ç®¡ç†å™¨
try:
    from common.http_proxy.proxy_manager import get_proxy_manager
except ImportError:
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä»é¡¹ç›®æ ¹ç›®å½•å¯¼å…¥
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    from common.http_proxy.proxy_manager import get_proxy_manager


class EnhancedCCXTProcessor:
    """
    å¢å¼ºç‰ˆçš„CCXTå¤„ç†å™¨ï¼Œç‹¬ç«‹äºFinRLå®ç°
    æ”¯æŒå¤šç§æ—¶é—´å‘¨æœŸå’Œæ›´å¥½çš„æ•°æ®è·å–ç­–ç•¥
    """

    def __init__(
        self,
        exchange_name="binance",
        market_type="spot",
        use_proxy: Optional[bool] = None,
        proxy_url: Optional[str] = None,
        timeout=3000,
    ):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆCCXTå¤„ç†å™¨

        å‚æ•°:
            exchange_name: äº¤æ˜“æ‰€åç§° ('binance', 'okx')
            market_type: å¸‚åœºç±»å‹ ('spot', 'swap')
            use_proxy: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨ä»£ç†ï¼ˆTrue=å¼ºåˆ¶å¼€å¯ï¼ŒFalse=å¼ºåˆ¶å…³é—­ï¼ŒNone=è‡ªåŠ¨æ£€æµ‹ï¼‰
            proxy_url: ä»£ç†æœåŠ¡å™¨åœ°å€ï¼ˆä»…åœ¨ use_proxy=True æ—¶ç”Ÿæ•ˆï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´(æ¯«ç§’)
        """
        print(f"åˆå§‹åŒ–CCXTå¤„ç†å™¨ (äº¤æ˜“æ‰€: {exchange_name}, å¸‚åœºç±»å‹: {market_type})...")
        self.exchange_name = exchange_name.lower()
        self.market_type = str(market_type).lower()
        self.use_proxy = use_proxy
        self.proxy_url = proxy_url

        # æ ¹æ®äº¤æ˜“æ‰€è·å–APIå¯†é’¥ç¯å¢ƒå˜é‡å
        if self.exchange_name == "okx":
            api_key = os.environ.get("OKX_API_KEY", "")
            api_secret = os.environ.get("OKX_API_SECRET", "")
            api_passphrase = os.environ.get("OKX_API_PASSPHRASE", "")
        else:  # binance
            api_key = os.environ.get("BINANCE_API_KEY", "")
            api_secret = os.environ.get("BINANCE_API_SECRET", "")
            api_passphrase = None

        # è®¾ç½®äº¤æ˜“æ‰€å®¢æˆ·ç«¯é…ç½®
        exchange_config = {
            "timeout": timeout,
            "enableRateLimit": True,
            "options": {
                "recvWindow": 60000,  # å¢åŠ æ¥æ”¶çª—å£æ—¶é—´
                "adjustForTimeDifference": True,  # è‡ªåŠ¨è°ƒæ•´æ—¶é—´å·®
                "keepAlive": True,  # ä¿æŒè¿æ¥æ´»è·ƒ
            },
            "headers": {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            },
        }

        # æ ¹æ®å¸‚åœºç±»å‹ä¸äº¤æ˜“æ‰€è®¾ç½® defaultTypeï¼ˆspot ä¸ swapï¼‰
        if market_type == "swap":
            # Binance æ°¸ç»­åœ¨ CCXT ä½¿ç”¨ 'future'; OKX æ°¸ç»­åœ¨ CCXT ä½¿ç”¨ 'swap'
            if self.exchange_name == "binance":
                exchange_config["options"]["defaultType"] = "future"
            elif self.exchange_name == "okx":
                exchange_config["options"]["defaultType"] = "swap"
            else:
                exchange_config["options"]["defaultType"] = "swap"
            print(
                f"å·²è®¾ç½®å¸‚åœºç±»å‹ä¸º: swap(æ°¸ç»­åˆçº¦) -> CCXT defaultType: {exchange_config['options']['defaultType']}"
            )
            # ä¸ºæ°¸ç»­åˆçº¦è®¾ç½®å¸¸è§é€‰é¡¹
            exchange_config["options"]["defaultMarginMode"] = "cross"
            exchange_config["options"]["createMarketBuyOrderRequiresPrice"] = False
            exchange_config["options"]["fetchTickerQuoteAsset"] = True
            exchange_config["options"]["broker"] = "CCXT"
            print("å·²ä¸ºæ°¸ç»­åˆçº¦é…ç½®é¢å¤–å‚æ•°")
        elif market_type == "spot":
            exchange_config["options"]["defaultType"] = "spot"
            print(
                f"å·²è®¾ç½®å¸‚åœºç±»å‹ä¸º: spot(ç°è´§) -> CCXT defaultType: {exchange_config['options']['defaultType']}"
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¸‚åœºç±»å‹: '{market_type}'ï¼Œä»…æ”¯æŒ spot(ç°è´§) æˆ– swap(æ°¸ç»­åˆçº¦)")

        # å¦‚æœAPIå¯†é’¥åœ¨ç¯å¢ƒå˜é‡ä¸­å­˜åœ¨
        if api_key and api_secret:
            exchange_config["apiKey"] = api_key
            exchange_config["secret"] = api_secret
            if api_passphrase:
                exchange_config["password"] = api_passphrase
            print(f"âœ… ä½¿ç”¨APIå¯†é’¥è¿æ¥{self.exchange_name}")
        else:
            print(f"âš ï¸ å…¬å¼€æ•°æ®æ¨¡å¼ï¼ˆæ— APIå¯†é’¥ï¼‰")

        # åˆå§‹åŒ–CCXTäº¤æ˜“æ‰€
        if self.exchange_name == "okx":
            self.exchange = ccxt.okx(exchange_config)
        else:  # binance
            self.exchange = ccxt.binance(exchange_config)
        print(f"âœ… {self.exchange_name}äº¤æ˜“æ‰€åˆå§‹åŒ–å®Œæˆ")

        # ä»£ç†è®¾ç½®ï¼šä¼˜å…ˆä½¿ç”¨æ˜¾å¼é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨ProxyManagerè‡ªåŠ¨æ¨¡å¼
        if self.use_proxy is True and self.proxy_url:
            print(f"è®¾ç½®ä»£ç†ï¼ˆæ‰‹åŠ¨æŒ‡å®šï¼‰: {self.proxy_url}")
            self.exchange.proxies = {"http": self.proxy_url, "https": self.proxy_url}
        elif self.use_proxy is False:
            print("ä¸ä½¿ç”¨ä»£ç†ï¼Œç›´è¿æ¨¡å¼")
            try:
                self.exchange.proxies = None
            except Exception:
                self.exchange.proxies = {}
            try:
                self.exchange.session.trust_env = False
            except Exception:
                pass
        else:
            print("ä½¿ç”¨ProxyManagerè‡ªåŠ¨æ£€æµ‹ä»£ç†é…ç½®...")
            proxy_manager = get_proxy_manager()
            proxy_config = proxy_manager.get_proxy_config()
            if proxy_config["use_proxy"] and proxy_config["proxies"]:
                self.exchange.proxies = proxy_config["proxies"]
                print(f"å·²åº”ç”¨ä»£ç†: {proxy_config['http_proxy']}")
            else:
                print("ProxyManageræ£€æµ‹ä¸ºç›´è¿æ¨¡å¼")
                try:
                    self.exchange.proxies = None
                except Exception:
                    self.exchange.proxies = {}
                try:
                    self.exchange.session.trust_env = False
                except Exception:
                    pass

        # è®¾ç½®è¯·æ±‚è¶…æ—¶å‚æ•°
        timeout_ms = int(max(3000, min(timeout, 15000)))
        self.exchange.timeout = timeout_ms
        self.exchange.httpOptions = {"timeout": timeout_ms, "keepAlive": True}

        # æ”¯æŒçš„æ—¶é—´å‘¨æœŸ
        self.supported_timeframes = ["1m", "3m", "5m", "15m", "30m", "1h", "4h", "1d"]

        # åŒæ­¥æ—¶é—´
        self._sync_time()

        # è¿æ¥æµ‹è¯•
        connection_success = self._test_connection()
        if not connection_success:
            print("âš ï¸ è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œæ•°æ®è·å–å¯èƒ½ä¼šå¤±è´¥")

    def _sync_time(self):
        """åŒæ­¥æœ¬åœ°æ—¶é—´ä¸æœåŠ¡å™¨æ—¶é—´"""
        try:
            server_time = self.exchange.fetch_time()
            if server_time is None:
                raise RuntimeError("æœªè·å–åˆ°æœåŠ¡å™¨æ—¶é—´")

            local_time = int(time.time() * 1000)
            time_diff = int(server_time) - local_time
            self.exchange.options["timeDifference"] = time_diff

            if abs(time_diff) > 1000:
                print(f"âš ï¸ æ—¶é—´å·®: {time_diff}ms (å·²è‡ªåŠ¨è°ƒæ•´)")
            else:
                print(f"âœ… æ—¶é—´åŒæ­¥æ­£å¸¸ (å·®å¼‚: {time_diff}ms)")

        except Exception as e:
            print(f"âš ï¸ æ—¶é—´åŒæ­¥å¤±è´¥: {e}")

    def format_date(self, year, month, day):
        """
        å°†å¹´æœˆæ—¥è½¬æ¢ä¸ºæ‰€éœ€çš„æ—¥æœŸæ ¼å¼

        å‚æ•°:
            year: å¹´ä»½ï¼Œå¦‚2024
            month: æœˆä»½ï¼Œå¦‚1
            day: æ—¥æœŸï¼Œå¦‚1

        è¿”å›:
            æ ¼å¼åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚"2024-01-01"
        """
        return f"{year}-{month:02d}-{day:02d}"

    def _test_connection(self):
        """æµ‹è¯•ä¸äº¤æ˜“æ‰€çš„è¿æ¥ï¼ˆæŒ‰äº¤æ˜“æ‰€/å¸‚åœºç±»å‹é€‰æ‹©å¯ç”¨ç¬¦å·ï¼‰"""
        try:
            # ç¡®ä¿åŠ è½½å¸‚åœº
            try:
                self.exchange.load_markets()
            except Exception:
                pass

            # ä¼˜å…ˆå€™é€‰åˆ—è¡¨
            candidate_symbols = []
            if self.exchange_name == "okx":
                if self.market_type == "swap":
                    candidate_symbols = ["ETH/USDT:USDT", "BTC/USDT:USDT", "ETH/USDT", "BTC/USDT"]
                else:
                    candidate_symbols = ["ETH/USDT", "BTC/USDT"]
            else:  # binance åŠå…¶ä»–
                if self.market_type == "swap":
                    candidate_symbols = ["ETH/USDT", "BTC/USDT"]
                else:
                    candidate_symbols = ["ETH/USDT", "BTC/USDT"]

            # åœ¨äº¤æ˜“æ‰€æ”¯æŒçš„ç¬¦å·ä¸­é€‰ä¸€ä¸ªå¯ç”¨çš„
            symbols = getattr(self.exchange, "symbols", None) or []
            chosen = None
            for s in candidate_symbols:
                if s in symbols:
                    chosen = s
                    break
            if chosen is None and symbols:
                chosen = symbols[0]

            # å®æµ‹è¿æ¥
            ticker = self.exchange.fetch_ticker(self._normalize_symbol(chosen or candidate_symbols[0]))
            last_price = ticker.get("last") if isinstance(ticker, dict) else None
            print(f"âœ… è¿æ¥æµ‹è¯•æˆåŠŸ (symbol: {chosen}, å½“å‰ä»·æ ¼: {last_price})")
            return True
        except Exception as e:
            print(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    def _normalize_symbol(self, symbol: str) -> str:
        """æŒ‰äº¤æ˜“æ‰€/å¸‚åœºç±»å‹è§„èŒƒåŒ–äº¤æ˜“å¯¹ç¬¦å·ï¼Œé¿å…å› ç¬¦å·ä¸ä¸€è‡´å¯¼è‡´è¯·æ±‚å¤±è´¥"""
        sym = str(symbol).upper().replace(" ", "")
        if self.exchange_name == "okx" and self.market_type == "swap":
            # OKX æ°¸ç»­åœ¨ CCXT ç»Ÿä¸€ä½¿ç”¨å½¢å¦‚ ETH/USDT:USDT
            if ":USDT" not in sym and sym.endswith("/USDT"):
                return sym + ":USDT"
        return sym

    def fetch_data(
        self, symbol, timeframe, start_date, end_date, retry_count=3, batch_size=1000, delay_ms=300
    ):
        """
        è·å–æŒ‡å®šäº¤æ˜“å¯¹å’Œæ—¶é—´èŒƒå›´çš„æ•°æ®å¹¶ä¿å­˜ä¸ºCSV

        å‚æ•°:
            symbol: äº¤æ˜“å¯¹åç§°ï¼Œå¦‚"ETH/USDT"
            timeframe: æ—¶é—´å‘¨æœŸï¼Œå¦‚"1d", "4h", "1h", "30m", "5m"
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸º"YYYY-MM-DD"
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º"YYYY-MM-DD"
            retry_count: å¤±è´¥é‡è¯•æ¬¡æ•°
            batch_size: æ¯æ‰¹è·å–çš„Kçº¿æ•°é‡
            delay_ms: è¯·æ±‚é—´éš”å»¶è¿Ÿ(æ¯«ç§’)

        è¿”å›:
            ä¿å­˜å¥½çš„æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        # æ£€æŸ¥æ—¶é—´å‘¨æœŸæ˜¯å¦æ”¯æŒ
        if timeframe not in self.supported_timeframes:
            print(f"è­¦å‘Š: æ—¶é—´å‘¨æœŸ {timeframe} ä¸åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ•°æ®ä¸å®Œæ•´")

        # è®¾ç½®ä¿å­˜è·¯å¾„
        raw_data_path = self._get_data_path(symbol)  # ä¾‹å¦‚: data/data_downloads/raw/eth

        # æ ¼å¼åŒ–æ—¥æœŸ
        start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
        end_datetime = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(
            days=1, seconds=-1
        )  # ç»“æŸæ—¶é—´è®¾ä¸ºå½“å¤©23:59:59
        # å¦‚æœç»ˆæ­¢æ—¥æœŸè¶…å‡ºå½“å‰æ—¶é—´ï¼Œè‡ªåŠ¨æˆªæ–­åˆ°å½“å‰æ—¶é—´
        now_dt = datetime.now()
        if end_datetime > now_dt:
            print("ç»“æŸæ—¥æœŸè¶…å‡ºå½“å‰æ—¶é—´ï¼Œå·²è‡ªåŠ¨æˆªæ–­åˆ°å½“å‰æ—¶é—´")
            end_datetime = now_dt

        # è·å–æ•°æ®
        print(f"\n=== å¼€å§‹è·å– {symbol} {timeframe} æ•°æ® ===")
        print(f"æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")

        # === ç»Ÿä¸€å‘½åè§„èŒƒï¼šç›´æ¥ä¿å­˜åˆ°ç›®æ ‡ä½ç½®ï¼Œæ ¼å¼ {SYMBOL}_{TIMEFRAME}.csv ===
        os.makedirs(raw_data_path, exist_ok=True)
        filename = f"{symbol.replace('/', '_')}_{timeframe}.csv"
        final_file_path = os.path.join(raw_data_path, filename)
        print(f"æ•°æ®å°†ä¿å­˜åˆ°: {os.path.abspath(final_file_path)}")
        # === å‘½åè§„èŒƒç»“æŸ ===

        # ä½¿ç”¨æ”¹è¿›çš„æ–¹æ³•è·å–æ•°æ®ï¼ˆå¯ç”¨åˆ†æ‰¹å¢é‡å†™å…¥ï¼‰
        df = self.download_data(
            symbol=symbol,
            start_date=start_datetime,
            end_date=end_datetime,
            time_interval=timeframe,
            retry_count=retry_count,
            batch_size=batch_size,
            delay_ms=delay_ms,
            output_file=final_file_path,
        )

        if df is not None and not df.empty:
            # æ·»åŠ æ—¥æœŸä¿¡æ¯å­—æ®µï¼Œæ–¹ä¾¿åç»­å¤„ç†
            df["date_str"] = df.index.strftime("%Y-%m-%d")
            df["time_str"] = df.index.strftime("%H:%M:%S")
            df["day_of_week"] = df.index.dayofweek
            # df['hour_of_day'] = df.index.hour
            # df['minute_of_hour'] = df.index.minute

            # æ£€æŸ¥æ•°æ®è´¨é‡
            missing_count = df.isnull().sum().sum()
            if missing_count > 0:
                print(f"âš ï¸ è­¦å‘Š: æ•°æ®ä¸­åŒ…å« {missing_count} ä¸ªç¼ºå¤±å€¼")
                # å¡«å……ç¼ºå¤±å€¼
                df = df.ffill().bfill()
                print("å·²å¡«å……ç¼ºå¤±å€¼")

            # æ£€æŸ¥é‡å¤ç´¢å¼•
            duplicate_count = df.index.duplicated().sum()
            if duplicate_count > 0:
                print(f"âš ï¸ è­¦å‘Š: æ•°æ®ä¸­åŒ…å« {duplicate_count} ä¸ªé‡å¤ç´¢å¼•")
                # åˆ é™¤é‡å¤è¡Œ
                df = df[~df.index.duplicated(keep="first")]
                print("å·²åˆ é™¤é‡å¤è¡Œ")

            # ä¿å­˜CSVæ–‡ä»¶ï¼ˆè‹¥å¯ç”¨åˆ†æ‰¹å†™å…¥ï¼Œdownload_data å·²å®Œæˆå†™å…¥ï¼›æ­¤å¤„è¦†ç›–ä¿å­˜ç”¨äºå…œåº•ç¡®ä¿ä¸€è‡´æ€§ï¼‰
            df.to_csv(final_file_path, index=True)
            print(f"âœ… æ•°æ®å·²ä¿å­˜è‡³ {final_file_path}")
            print(f"æ—¶é—´èŒƒå›´: {df.index[0]} - {df.index[-1]}")
            print(f"æ€»è®°å½•æ•°: {len(df)}")

            # åˆ†ææ•°æ®å®Œæ•´æ€§
            expected_intervals = self._calculate_expected_intervals(start_datetime, end_datetime, timeframe)
            completeness = min(100.0, (len(df) / expected_intervals) * 100) if expected_intervals > 0 else 0
            print(f"æ•°æ®å®Œæ•´æ€§: {completeness:.2f}% (é¢„æœŸè®°å½•æ•°: {expected_intervals})")

            return final_file_path  # è¿”å›æ–°çš„æ–‡ä»¶è·¯å¾„
        else:
            print("âŒ è·å–æ•°æ®å¤±è´¥")
            return None

    def _calculate_expected_intervals(self, start_date, end_date, timeframe):
        """è®¡ç®—æŒ‡å®šæ—¶é—´èŒƒå›´å†…åº”æœ‰çš„Kçº¿æ•°é‡"""
        # æ—¶é—´å‘¨æœŸè½¬æ¢ä¸ºåˆ†é’Ÿ
        tf_minutes = self._timeframe_to_minutes(timeframe)
        if tf_minutes == 0:  # å¯¹äºæ—¥çº¿ç­‰æƒ…å†µ
            return 0

        # è®¡ç®—æ€»åˆ†é’Ÿæ•°
        total_minutes = (end_date - start_date).total_seconds() / 60
        # è€ƒè™‘å¸‚åœºå¼€æ”¾æ—¶é—´ (åŠ å…¥è¿™ä¸ªå› ç´ ä¼šæ›´å‡†ç¡®ï¼Œä½†ä¸ºç®€åŒ–æš‚æ—¶å¿½ç•¥)
        # å¸å®‰æ˜¯24/7äº¤æ˜“ï¼Œæ‰€ä»¥ç›´æ¥è®¡ç®—
        return total_minutes / tf_minutes

    def _timeframe_to_minutes(self, timeframe):
        """å°†æ—¶é—´å‘¨æœŸè½¬æ¢ä¸ºåˆ†é’Ÿæ•°"""
        if timeframe.endswith("m"):
            return int(timeframe[:-1])
        elif timeframe.endswith("h"):
            return int(timeframe[:-1]) * 60
        elif timeframe.endswith("d"):
            return int(timeframe[:-1]) * 60 * 24
        return 0

    def _get_data_path(self, symbol: str):
        """è·å–æ•°æ®ä¿å­˜è·¯å¾„ï¼Œå¹¶åœ¨rawä¸‹ä¸ºsymbolåˆ›å»ºå­ç›®å½•"""
        # æå–äº¤æ˜“å¯¹çš„åŸºç¡€åç§°
        symbol_lower = symbol.lower()
        if "/" in symbol_lower:
            base_symbol = symbol_lower.split("/")[0]
        elif symbol_lower.endswith("usdt"):
            base_symbol = symbol_lower[:-4]
        else:
            base_symbol = symbol_lower

        # ç¡®å®šæ ¹æ•°æ®ç›®å½• - ä¿®æ”¹è¿™é‡Œ
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        data_root = os.path.join(project_root, "data", "data_downloads", "raw")  # ä¿®æ”¹åçš„è·¯å¾„

        # æ£€æŸ¥æ ¹æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        if not os.path.exists(data_root):
            # å¦‚æœæ ¹ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•ä¸Šä¸€çº§ï¼ˆå¯èƒ½è„šæœ¬åœ¨å­ç›®å½•è¿è¡Œï¼‰
            parent_root = os.path.dirname(project_root)
            # å°è¯•æ–°çš„æ›¿ä»£è·¯å¾„
            data_root_alt = os.path.join(parent_root, "data", "data_downloads", "raw")
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ data/data_downloads ç›®å½•
            if os.path.exists(os.path.join(parent_root, "data", "data_downloads")):
                data_root = data_root_alt
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œåˆ™åœ¨å½“å‰å·¥ä½œç›®å½•ä¸‹åˆ›å»º
            elif not os.path.exists(data_root):
                data_root = os.path.join(os.getcwd(), "data", "data_downloads", "raw")
            os.makedirs(data_root, exist_ok=True)

        # æ„å»ºç‰¹å®šäº¤æ˜“å¯¹çš„å­ç›®å½•è·¯å¾„
        symbol_specific_path = os.path.join(data_root, base_symbol)

        # ç¡®ä¿å­ç›®å½•å­˜åœ¨
        os.makedirs(symbol_specific_path, exist_ok=True)

        print(f"æ•°æ®å°†ä¿å­˜åˆ°ç‰¹å®šå­ç›®å½•: {os.path.abspath(symbol_specific_path)}")
        return symbol_specific_path

    def download_data(
        self,
        symbol,
        start_date,
        end_date,
        time_interval,
        retry_count=5,
        batch_size=500,
        delay_ms=500,
        output_file: str | None = None,
    ):
        """
        æ”¹è¿›çš„æ•°æ®ä¸‹è½½æ–¹æ³•ï¼Œæä¾›æ›´è¯¦ç»†çš„è¿›åº¦ä¿¡æ¯å’Œæ›´å¥½çš„å¼‚å¸¸å¤„ç†

        å‚æ•°:
            symbol: äº¤æ˜“å¯¹
            start_date: å¼€å§‹æ—¥æœŸ datetimeå¯¹è±¡
            end_date: ç»“æŸæ—¥æœŸ datetimeå¯¹è±¡
            time_interval: æ—¶é—´å‘¨æœŸ
            retry_count: å¤±è´¥é‡è¯•æ¬¡æ•°
            batch_size: æ¯æ‰¹è·å–çš„Kçº¿æ•°é‡ï¼ˆè‡ªåŠ¨ä¼˜åŒ–ï¼‰
            delay_ms: è¯·æ±‚é—´éš”å»¶è¿Ÿ(æ¯«ç§’ï¼Œè‡ªåŠ¨ä¼˜åŒ–)

        è¿”å›:
            DataFrame åŒ…å«OHLCVæ•°æ®
        """
        # ğŸš€ æ ¹æ®äº¤æ˜“æ‰€è‡ªåŠ¨ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
        if self.exchange_name == "okx":
            # OKXå†å²æ•°æ®é™åˆ¶ï¼šæœ€å¤š300æ ¹
            batch_size = min(batch_size, 300)
        elif self.exchange_name == "binance":
            # Binanceå¯ä»¥æ”¯æŒ1000æ ¹
            batch_size = min(batch_size, 1000)

        # ğŸš€ è‡ªåŠ¨ä¼˜åŒ–å»¶è¿Ÿï¼ˆç¡®ä¿ä¸è§¦å‘é™æµï¼‰
        if delay_ms < 100:
            print(f"âš ï¸ å»¶è¿Ÿè¿‡ä½({delay_ms}ms)ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º100msä»¥é¿å…é™æµ")
            delay_ms = 100
        # è½¬æ¢ä¸ºæ—¶é—´æˆ³ (æ¯«ç§’)
        since = int(start_date.timestamp() * 1000)
        until = int(end_date.timestamp() * 1000)

        print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½: {symbol} {time_interval}")
        print(f"â° æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        print(
            f"ğŸš€ ä¼˜åŒ–å‚æ•°: {batch_size}æ ¹/æ‰¹, {delay_ms}mså»¶è¿Ÿ (é€Ÿåº¦: ~{int(batch_size * 1000 / delay_ms * 60)}æ ¹/åˆ†é’Ÿ)"
        )

        # è¿”å›å®Œæ•´æ•°æ®çš„åˆ—è¡¨
        all_candles = []

        # å½“å‰æ—¶é—´æˆ³
        current_since = since

        # è®¡æ•°å™¨
        total_fetched = 0
        batch_count = 0
        consecutive_failures = 0

        # è®¡ç®—é¢„æœŸæ€»æ‰¹æ¬¡
        expected_batches = self._estimate_batches(since, until, time_interval, limit=batch_size)
        print(f"é¢„è®¡éœ€è¦è·å–çº¦ {expected_batches} æ‰¹æ•°æ®")

        # å¢é‡å†™å…¥æ§åˆ¶
        header_written = False
        if output_file:
            header_written = os.path.exists(output_file)

        # å¾ªç¯è·å–æ‰€æœ‰æ•°æ®
        while current_since < until:
            retry = 0
            success = False
            no_data_this_round = False

            while retry < retry_count and not success:
                try:
                    if batch_count % 5 == 0 or batch_count == 0:
                        print(
                            f"è¯·æ±‚æ‰¹æ¬¡ #{batch_count + 1}: ä» {datetime.fromtimestamp(current_since / 1000)}"
                        )
                    else:
                        print(".", end="", flush=True)  # ç®€åŒ–çš„è¿›åº¦æ˜¾ç¤º

                    # é‡æ–°è®¡ç®—æ¯æ‰¹è·å–æ•°é‡ï¼Œç¡®ä¿æœ€åä¸€æ‰¹ä¸ä¼šè¶…å‡ºç»“æŸæ—¶é—´
                    remaining_time = until - current_since
                    tf_ms = self._timeframe_to_ms(time_interval)
                    remaining_candles = remaining_time / tf_ms
                    current_limit = min(batch_size, int(remaining_candles) + 10)  # é¢å¤–è·å–å‡ æ¡ä»¥ç¡®ä¿è¦†ç›–

                    # è·å–Kçº¿æ•°æ®
                    candles = self.exchange.fetch_ohlcv(
                        symbol=self._normalize_symbol(symbol),
                        timeframe=time_interval,
                        since=current_since,
                        limit=current_limit,
                    )

                    if not candles or len(candles) == 0:
                        print("\næ²¡æœ‰è·å–åˆ°æ•°æ®ï¼Œå¯èƒ½å·²åˆ°è¾¾æ•°æ®æœ«å°¾æˆ–æŒ‡å®šæ—¶é—´å†…æ— äº¤æ˜“")
                        no_data_this_round = True
                        break

                    # æ·»åŠ åˆ°æ€»åˆ—è¡¨
                    all_candles.extend(candles)

                    # æ›´æ–°è®¡æ•°
                    current_batch_size = len(candles)
                    total_fetched += current_batch_size
                    batch_count += 1
                    consecutive_failures = 0  # é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°

                    # è·å–æœ€åä¸€ä¸ªKçº¿çš„æ—¶é—´ä½œä¸ºä¸‹ä¸€æ‰¹çš„å¼€å§‹
                    current_since = candles[-1][0] + 1  # +1æ¯«ç§’é¿å…é‡å¤

                    # åˆ†æ‰¹å¢é‡å†™å…¥åˆ°CSVï¼Œå‡å°‘é•¿ä»»åŠ¡ä¸­çš„æ•°æ®ä¸¢å¤±é£é™©
                    if output_file:
                        try:
                            batch_df = pd.DataFrame(
                                candles, columns=["timestamp", "open", "high", "low", "close", "volume"]
                            )
                            batch_df["timestamp"] = pd.to_datetime(batch_df["timestamp"], unit="ms")
                            write_mode = "a" if header_written else "w"
                            batch_df.to_csv(
                                output_file, index=False, mode=write_mode, header=not header_written
                            )
                            header_written = True
                        except Exception as werr:
                            print(f"å¢é‡å†™å…¥å¤±è´¥: {werr}")

                    # æ˜¾ç¤ºå½“å‰è¿›åº¦ï¼ˆæ¯5æ‰¹æˆ–ç´¯è®¡è¶…è¿‡ç‰¹å®šæ•°é‡æ—¶ï¼‰
                    if batch_count % 5 == 0 or batch_count == 1:
                        datetime.fromtimestamp(candles[-1][0] / 1000)
                        progress = min(100, round((current_since - since) / (until - since) * 100))
                        print(
                            f"\næ‰¹æ¬¡ {batch_count}/{expected_batches} ({progress}%): "
                            f"è·å–äº† {current_batch_size} æ¡Kçº¿ï¼Œç´¯è®¡: {total_fetched} æ¡"
                        )

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    self.exchange.sleep(delay_ms)

                    # æ ‡è®°æˆåŠŸ
                    success = True

                except ccxt.AuthenticationError as auth_err:
                    retry += 1
                    print(f"\nè·å–æ•°æ®æ—¶å‘ç”Ÿè®¤è¯é”™è¯¯ (å°è¯• {retry}/{retry_count}): {auth_err}")
                    print("è¯·æ£€æŸ¥APIå¯†é’¥æƒé™å’ŒIPç™½åå•è®¾ç½®ã€‚")
                    if retry >= retry_count:
                        print("è®¤è¯é”™è¯¯è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒæ•°æ®è·å–ã€‚")
                        return None
                    self.exchange.sleep(2000)  # ç­‰å¾…2ç§’å†è¯•

                except ccxt.DDoSProtection as ddos_err:
                    retry += 1
                    wait_time = retry * 3  # é™ä½ç­‰å¾…æ—¶é•¿
                    print(f"\nDDoSä¿æŠ¤æœºåˆ¶è§¦å‘ (å°è¯• {retry}/{retry_count}): {ddos_err}")
                    print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    self.exchange.sleep(wait_time * 1000)

                except ccxt.ExchangeNotAvailable as not_avail_err:
                    retry += 1
                    wait_time = retry * 5  # é™ä½ç­‰å¾…æ—¶é•¿
                    print(f"\näº¤æ˜“æ‰€ä¸å¯ç”¨ (å°è¯• {retry}/{retry_count}): {not_avail_err}")
                    print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    self.exchange.sleep(wait_time * 1000)

                except ccxt.RequestTimeout as timeout_err:
                    retry += 1
                    wait_time = retry * 2  # é™ä½ç­‰å¾…æ—¶é•¿
                    print(f"\nè¯·æ±‚è¶…æ—¶ (å°è¯• {retry}/{retry_count}): {timeout_err}")
                    print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    self.exchange.sleep(wait_time * 1000)

                except Exception as e:
                    retry += 1
                    print(f"\nè·å–æ•°æ®å‡ºé”™ (å°è¯• {retry}/{retry_count}): {e}")
                    print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")

                    if retry < retry_count:
                        wait_time = retry * 5  # é€’å¢ç­‰å¾…æ—¶é—´
                        print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        self.exchange.sleep(wait_time * 1000)
                    else:
                        print("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                        # å¦‚æœæ˜¯ç¬¬ä¸€æ‰¹æ¬¡å°±å¤±è´¥ï¼Œç›´æ¥é€€å‡º
                        if batch_count == 0:
                            print("âŒ ç¬¬ä¸€æ‰¹æ¬¡å°±å¤±è´¥ï¼Œå¯èƒ½æ˜¯ç½‘ç»œæˆ–é…ç½®é—®é¢˜ï¼Œåœæ­¢æ•°æ®è·å–")
                            return None
                        else:
                            print("å°è¯•è·³è¿‡å½“å‰æ‰¹æ¬¡")
                            # å°è¯•å‘å‰æ¨è¿›æ—¶é—´æˆ³
                            tf_ms = self._timeframe_to_ms(time_interval)
                            current_since += tf_ms * min(batch_size // 2, 10)  # è·³è¿‡ä¸€äº›Kçº¿ï¼Œä½†ä¸è¦å¤ªå¤š
                            consecutive_failures += 1

            # è¿‘å°¾éƒ¨æ— æ•°æ®çš„æå‰ç»ˆæ­¢ï¼šå¦‚æœæ¥è¿‘ç»“æŸä¸”æœ¬è½®æ— æ•°æ®ï¼Œç›´æ¥è·³å‡º
            if no_data_this_round:
                tf_ms = self._timeframe_to_ms(time_interval)
                remaining_time = until - current_since
                if remaining_time <= tf_ms * 3:
                    print("æ¥è¿‘æ—¶é—´èŒƒå›´æœ«å°¾ä¸”æ— æ•°æ®ï¼Œæå‰ç»“æŸå¾ªç¯")
                    break

            # å¦‚æœè¿ç»­å¤šæ¬¡æ‰¹æ¬¡å¤±è´¥ï¼Œæš‚åœä¸€æ®µæ—¶é—´æˆ–å‡å°æ‰¹é‡å¤§å°
            if not success:
                consecutive_failures += 1

                if consecutive_failures >= 3:
                    print(f"\nè­¦å‘Š: è¿ç»­ {consecutive_failures} æ¬¡æ‰¹æ¬¡è·å–å¤±è´¥")
                    print("å¯èƒ½æ˜¯é‡åˆ°äº†APIé™åˆ¶æˆ–æ•°æ®ç¨€ç–åŒºåŸŸ")

                    if consecutive_failures >= 5:
                        print("è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæš‚åœ20ç§’åç»§ç»­ï¼ˆä¿æŒæ‰¹é‡å¤§å°ä¸å˜ï¼‰...")
                        self.exchange.sleep(20000)
                    else:
                        print("æš‚åœ10ç§’åç»§ç»­...")
                        self.exchange.sleep(10000)

        if not all_candles:
            print("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")
            return None

        print(f"\næ•°æ®è·å–å®Œæˆ: æ€»å…± {len(all_candles)} æ¡Kçº¿")

        # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(all_candles, columns=["timestamp", "open", "high", "low", "close", "volume"])

        # è½¬æ¢æ—¶é—´æˆ³
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
        df.set_index("timestamp", inplace=True)

        # åˆ é™¤é‡å¤è¡Œ
        df = df[~df.index.duplicated(keep="first")]

        # æ’åº
        df.sort_index(inplace=True)

        print(f"æ•°æ®å¤„ç†å®Œæˆ: {symbol} {time_interval}")
        print(f"æ—¶é—´èŒƒå›´: {df.index[0]} è‡³ {df.index[-1]}")
        print(f"æ€»è®°å½•æ•°: {len(df)}")

        return df

    def _timeframe_to_ms(self, timeframe):
        """å°†æ—¶é—´å‘¨æœŸè½¬æ¢ä¸ºæ¯«ç§’æ•°"""
        minutes = self._timeframe_to_minutes(timeframe)
        return minutes * 60 * 1000

    def _estimate_batches(self, since, until, timeframe, limit):
        """ä¼°è®¡éœ€è¦çš„æ‰¹æ¬¡æ•°é‡"""
        # è®¡ç®—æ€»æ—¶é—´èŒƒå›´(æ¯«ç§’)
        time_range_ms = until - since

        # è®¡ç®—å•ä¸ªæ—¶é—´å‘¨æœŸçš„æ¯«ç§’æ•°
        tf_ms = self._timeframe_to_ms(timeframe)

        if tf_ms == 0:
            return 0

        # ä¼°è®¡Kçº¿æ€»æ•°
        estimated_candles = time_range_ms / tf_ms

        # ä¼°è®¡æ‰¹æ¬¡æ•°
        estimated_batches = estimated_candles / limit

        return int(estimated_batches) + 1

    def fetch_data_for_days(self, symbol, timeframe, days):
        """
        è·å–æœ€è¿‘Nå¤©çš„æ•°æ®

        å‚æ•°:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            days: å¤©æ•°

        è¿”å›:
            ä¿å­˜å¥½çš„æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        today = datetime.now()
        start_date = (today - timedelta(days=days)).strftime("%Y-%m-%d")
        end_date = today.strftime("%Y-%m-%d")

        return self.fetch_data(symbol, timeframe, start_date, end_date)

    def fetch_data_by_year(self, symbol, timeframe, year):
        """
        è·å–æŒ‡å®šå¹´ä»½çš„æ•°æ®

        å‚æ•°:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            year: å¹´ä»½ï¼Œå¦‚2023

        è¿”å›:
            ä¿å­˜å¥½çš„æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        start_date = f"{year}-01-01"

        # å¦‚æœæ˜¯å½“å‰å¹´ä»½ï¼Œåˆ™åªè·å–åˆ°å½“å‰æ—¥æœŸ
        if year == datetime.now().year:
            end_date = datetime.now().strftime("%Y-%m-%d")
        else:
            end_date = f"{year}-12-31"

        return self.fetch_data(symbol, timeframe, start_date, end_date)

    def fetch_multi_timeframe_data(self, symbol, timeframes=None, start_date=None, end_date=None, days=365):
        """
        è·å–å¤šä¸ªæ—¶é—´å‘¨æœŸçš„æ•°æ®

        å‚æ•°:
            symbol: äº¤æ˜“å¯¹
            timeframes: æ—¶é—´å‘¨æœŸåˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰æ”¯æŒçš„
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸º"YYYY-MM-DD"ï¼Œé»˜è®¤æ ¹æ®dayså‚æ•°è®¡ç®—
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º"YYYY-MM-DD"ï¼Œé»˜è®¤ä¸ºä»Šå¤©
            days: å¦‚æœªæŒ‡å®šå¼€å§‹æ—¥æœŸï¼Œåˆ™è·å–æœ€è¿‘dayså¤©çš„æ•°æ®

        è¿”å›:
            ä¿å­˜å¥½çš„æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # å¦‚æœæœªæŒ‡å®šæ—¶é—´å‘¨æœŸï¼Œä½¿ç”¨æ‰€æœ‰æ”¯æŒçš„
        if timeframes is None:
            timeframes = self.supported_timeframes

        # è®¾ç½®ç»“æŸæ—¥æœŸ
        if end_date is None:
            end_date = datetime.now().strftime("%Y-%m-%d")

        # è®¾ç½®å¼€å§‹æ—¥æœŸ
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")

        print(f"\n=== å¼€å§‹è·å– {symbol} çš„å¤šæ—¶é—´å‘¨æœŸæ•°æ® ===")
        print(f"æ—¶é—´å‘¨æœŸ: {', '.join(timeframes)}")
        print(f"æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")

        # åˆ›å»ºåˆç†çš„æ•°æ®è·å–è®¡åˆ’ - ä¸åŒå‘¨æœŸè·å–ä¸åŒçš„å†å²é•¿åº¦
        fetch_plan = self._create_fetch_plan(timeframes, start_date, end_date)

        # æ‰§è¡Œè·å–è®¡åˆ’
        results = []
        for tf, dates in fetch_plan.items():
            tf_start = dates["start"]
            tf_end = dates["end"]
            print(f"\nå¼€å§‹è·å– {tf} å‘¨æœŸæ•°æ®: {tf_start} è‡³ {tf_end}")
            file_path = self.fetch_data(symbol, tf, tf_start, tf_end)
            if file_path:
                results.append(file_path)

        return results

    def update_base_csv(
        self,
        symbol: str,
        base_tf: str = "5m",
        output_dir: str | None = None,
        days_if_missing: int = 60,
        fill_missing: bool = True,
        initial_start_str: str | None = None,
        symbol_std_override: str | None = None,
    ):
        """
        å¢é‡æ›´æ–°ä»»æ„åŸºç¡€å‘¨æœŸæ•°æ®ï¼ˆå¦‚ 1m/3m/5m/15m ç­‰ï¼‰ï¼Œå¹¶ä¸æœ¬åœ°å·²æœ‰æ–‡ä»¶åˆå¹¶ï¼›
        ä¸‹è½½è¿‡ç¨‹ä¸­å°†æ‰¹æ¬¡æ•°æ®ç›´æ¥è¿½åŠ å†™å…¥ç›®æ ‡ CSVï¼ˆè¾¹ä¸‹è¾¹å­˜ï¼‰ã€‚

        å‚æ•°:
            symbol: äº¤æ˜“å¯¹ï¼Œå¦‚ "ETH/USDT"
            base_tf: åŸºç¡€å‘¨æœŸï¼Œå¦‚ "1m" / "5m"
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ data/data_downloads
            days_if_missing: å¦‚æœæœ¬åœ°ä¸å­˜åœ¨æ–‡ä»¶ï¼Œåˆæ¬¡æŠ“å–çš„å¤©æ•°
            fill_missing: æ˜¯å¦æŒ‰åŸºç¡€å‘¨æœŸè¡¥é½æ—¶é—´ç½‘æ ¼
            initial_start_str: åˆå§‹èµ·ç‚¹(ä»…å½“æœ¬åœ°æ— æ–‡ä»¶æ—¶ç”Ÿæ•ˆ)
            symbol_std_override: ğŸ”¥ å¤–éƒ¨ä¼ å…¥çš„æ ‡å‡†åŒ–å¸ç§åï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰ï¼Œä¼˜å…ˆçº§æœ€é«˜

        è¿”å›:
            æœ€ç»ˆå†™å…¥çš„ CSV ç»å¯¹è·¯å¾„
        """
        # è®¡ç®—è¾“å‡ºç›®å½•ï¼ˆå— main.io æ§åˆ¶ï¼›ç¼ºå¤±æ—¶åŸºäº base_dir æ¨å¯¼ï¼‰
        if output_dir is None:
            try:
                from .congfigs.config_loader import ConfigLoader as _CL  # ç›¸å¯¹å¯¼å…¥
            except ImportError:
                from features_engineering.congfigs.config_loader import ConfigLoader as _CL  # ç»å¯¹å¯¼å…¥
            _loader = _CL()
            _main_cfg = _loader.load_main_config() or {}
            _io = _main_cfg.get("io", {}) or {}
            _base_dir = _io.get("base_dir") or os.path.join(os.path.expanduser("~"), "FinRL_bn", "data")
            output_dir = _io.get("downloads_dir") or f"{_base_dir}/rl_live/data_downloads"
        output_dir = os.path.abspath(output_dir)
        os.makedirs(output_dir, exist_ok=True)

        # ğŸ”¥ ç›®æ ‡æ–‡ä»¶åä¼˜å…ˆçº§ï¼š
        # 1. symbol_std_overrideï¼ˆå¤–æŒ‚ä¼ å…¥ï¼Œæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        # 2. ä» symbol å‚æ•°æ¨å¯¼ï¼ˆETH/USDT â†’ ETH_USDTï¼‰
        # æ³¨æ„ï¼šä¸å†ä» main_config è¯»å–ï¼Œé¿å…å¤šå¸ç§ä¸‹è½½æ—¶æ–‡ä»¶åæ··æ·†
        if symbol_std_override:
            symbol_for_filename = symbol_std_override
        else:
            # ä»äº¤æ˜“æ‰€æ ¼å¼æ¨å¯¼ï¼šETH/USDT:USDT â†’ ETH_USDT
            symbol_for_filename = symbol.replace("/", "_").replace(":USDT", "").replace(":USD", "")
        
        market_tag = (self.market_type or "").upper() if isinstance(self.market_type, str) else ""
        filename = f"{symbol_for_filename}_{market_tag}_{base_tf}.csv"
        output_path = os.path.join(output_dir, filename)

        print(f"\n=== æ›´æ–° {base_tf} èšåˆæ•°æ®æ–‡ä»¶ ===")
        print(f"ç›®æ ‡æ–‡ä»¶: {output_path}")

        # å°è¯•è¯»å–å·²å­˜åœ¨çš„æ•°æ®
        existing_df = None
        if os.path.exists(output_path):
            try:
                existing_df = pd.read_csv(output_path, parse_dates=[0], index_col=0)
                existing_df.index.name = "timestamp"
                keep_cols = [
                    c for c in ["open", "high", "low", "close", "volume"] if c in existing_df.columns
                ]
                existing_df = existing_df[keep_cols]
                print(
                    f"å·²è¯»å–æœ¬åœ°å†å²æ•°æ®: {len(existing_df)} æ¡, æ—¶é—´èŒƒå›´: {existing_df.index.min()} ~ {existing_df.index.max()}"
                )
            except Exception as e:
                print(f"è¯»å–æœ¬åœ°æ–‡ä»¶å¤±è´¥ï¼Œå°†é‡æ–°æ„å»º: {e}")
                existing_df = None

        now_dt = datetime.now()

        # è§£æåˆå§‹èµ·ç‚¹å­—ç¬¦ä¸²ï¼ˆåœ¨æœ¬åœ°æ— å†å²æ–‡ä»¶æ—¶ä½¿ç”¨ï¼‰
        def _parse_initial_start(s: str) -> datetime:
            for fmt in ("%Y-%m-%d-%H-%M", "%Y-%m-%d %H:%M", "%Y-%m-%d"):
                try:
                    return datetime.strptime(s, fmt)
                except Exception:
                    pass
            raise ValueError(f"æ— æ³•è§£æ initial_start_str: {s}")

        # è®¡ç®—å¼€å§‹æŠ“å–æ—¶é—´ï¼ˆå‘å‰å›æº¯5ä¸ªåŸºç¡€å‘¨æœŸï¼Œé¿å…ç¼ºå£ï¼‰
        base_minutes = self._timeframe_to_minutes(base_tf) or 1
        backtrack = timedelta(minutes=base_minutes * 5)
        step_delta = timedelta(minutes=base_minutes)

        if existing_df is not None and not existing_df.empty:
            last_ts = existing_df.index.max()
            start_dt = (last_ts + step_delta) - backtrack
            print(f"å¢é‡æ›´æ–°èµ·ç‚¹: {start_dt} (æœ¬åœ°æœ€åä¸€æ¡: {last_ts})")
        else:
            if initial_start_str:
                start_dt = _parse_initial_start(initial_start_str)
                print(f"æœ¬åœ°æ— å†å²æ–‡ä»¶ï¼ŒæŒ‰å›ºå®šèµ·ç‚¹æŠ“å–ï¼Œèµ·ç‚¹: {start_dt}")
            else:
                start_dt = now_dt - timedelta(days=days_if_missing)
                print(f"æœ¬åœ°æ— å†å²æ–‡ä»¶ï¼Œé¦–æ¬¡æŠ“å–è¿‘ {days_if_missing} å¤©æ•°æ®ï¼Œèµ·ç‚¹: {start_dt}")

        # è‹¥èµ·æ­¢æ— æ•ˆåˆ™ç›´æ¥ä¿å­˜ç°æœ‰æ•°æ®ï¼ˆå¯é€‰è¡¥é½ï¼‰
        if start_dt >= now_dt:
            print("æœ¬åœ°æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€å¢é‡æŠ“å–")
            final_df = existing_df
        else:
            incr_df = self.download_data(
                symbol=symbol,
                start_date=start_dt,
                end_date=now_dt,
                time_interval=base_tf,
                output_file=output_path,
            )

            if incr_df is None or incr_df.empty:
                print("æœªè·å–åˆ°å¢é‡æ•°æ®ï¼Œä½¿ç”¨æœ¬åœ°å·²æœ‰æ•°æ®")
                final_df = existing_df
            else:
                print(
                    f"è·å–åˆ°å¢é‡: {len(incr_df)} æ¡, æ—¶é—´èŒƒå›´: {incr_df.index.min()} ~ {incr_df.index.max()}"
                )
                if existing_df is not None and not existing_df.empty:
                    final_df = pd.concat([existing_df, incr_df])
                else:
                    final_df = incr_df

        if final_df is None or final_df.empty:
            print("âŒ æ— å¯å†™å…¥çš„æ•°æ®")
            return None

        # å»é‡ã€æ’åº
        final_df = final_df[~final_df.index.duplicated(keep="last")].sort_index()

        # å¯é€‰: æŒ‰åŸºç¡€å‘¨æœŸè¡¥é½æ—¶é—´ç½‘æ ¼
        if fill_missing:
            # å°† base_tf è½¬æ¢ä¸º pandas é¢‘ç‡è§„åˆ™
            unit = base_tf.strip().lower()[-1]
            num = int(base_tf[:-1]) if base_tf[:-1].isdigit() else 1
            if unit == "m":
                freq = f"{num}min"
            elif unit == "h":
                freq = f"{num}h"
            elif unit == "d":
                freq = f"{num}d"
            elif unit == "w":
                freq = f"{num}w"
            else:
                freq = "1min"
            full_index = pd.date_range(start=final_df.index.min(), end=final_df.index.max(), freq=freq)
            final_df = final_df.reindex(full_index)
            final_df.index.name = "timestamp"

            if "close" in final_df.columns:
                final_df["close"] = final_df["close"].ffill()
            for col in ["open", "high", "low"]:
                if col in final_df.columns:
                    if "close" in final_df.columns:
                        final_df[col] = final_df[col].fillna(final_df["close"])
                    else:
                        final_df[col] = final_df[col].ffill()
            if "volume" in final_df.columns:
                final_df["volume"] = final_df["volume"].fillna(0.0)

        # ä¿å­˜
        final_df.to_csv(output_path, index=True)
        print(f"âœ… å·²å†™å…¥: {output_path}")
        print(f"æœ€ç»ˆæ—¶é—´èŒƒå›´: {final_df.index.min()} ~ {final_df.index.max()}  (å…± {len(final_df)} æ¡)")

        # ä½¿ç”¨ç»Ÿä¸€çš„å·¥å…·å‡½æ•°æ‰“å°æœ€æ–°æ—¶é—´
        from tools.io_paths import print_latest_timestamp_from_df

        print_latest_timestamp_from_df(final_df)

        return output_path

    def _create_fetch_plan(self, timeframes, start_date, end_date):
        """
        åˆ›å»ºåˆç†çš„æ•°æ®è·å–è®¡åˆ’ï¼Œä¸ºä¸åŒå‘¨æœŸè®¾ç½®ä¸åŒçš„å†å²é•¿åº¦

        å‚æ•°:
            timeframes: æ—¶é—´å‘¨æœŸåˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸º"YYYY-MM-DD"
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º"YYYY-MM-DD"
        """
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")

        # åˆ›å»ºè·å–è®¡åˆ’å­—å…¸
        plan = {}

        # ä¸ºä¸åŒæ—¶é—´å‘¨æœŸè®¾ç½®ä¸åŒçš„å†å²é•¿åº¦
        for tf in timeframes:
            tf_start = start_dt

            # å°å‘¨æœŸå‡å°‘å†å²é•¿åº¦ï¼Œå‡è½»å­˜å‚¨å‹åŠ›
            if tf == "1m":
                # 1åˆ†é’Ÿæ•°æ®æœ€å¤šè·å–30å¤©
                days_to_fetch = min(30, (end_dt - start_dt).days)
                tf_start = end_dt - timedelta(days=days_to_fetch)
            elif tf == "3m":
                # 3åˆ†é’Ÿæ•°æ®æœ€å¤šè·å–60å¤©
                days_to_fetch = min(60, (end_dt - start_dt).days)
                tf_start = end_dt - timedelta(days=days_to_fetch)
            elif tf == "5m":
                # 5åˆ†é’Ÿæ•°æ®æœ€å¤šè·å–90å¤©
                days_to_fetch = min(90, (end_dt - start_dt).days)
                tf_start = end_dt - timedelta(days=days_to_fetch)
            elif tf == "15m" or tf == "30m":
                # 15å’Œ30åˆ†é’Ÿæ•°æ®æœ€å¤šè·å–180å¤©
                days_to_fetch = min(180, (end_dt - start_dt).days)
                tf_start = end_dt - timedelta(days=days_to_fetch)

            plan[tf] = {"start": tf_start.strftime("%Y-%m-%d"), "end": end_date}

        return plan

    def fetch_all_timeframes(self, symbol, years=2):
        """
        æŒ‰ç…§æœ€ä¼˜å®è·µè·å–æ‰€æœ‰æ—¶é—´å‘¨æœŸçš„æ•°æ®

        å‚æ•°:
            symbol: äº¤æ˜“å¯¹
            years: è·å–å¤šå°‘å¹´çš„æ•°æ®

        è¿”å›:
            ä¿å­˜å¥½çš„æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=years * 365)).strftime("%Y-%m-%d")

        # ä¸ºæ¯ä¸ªæ—¶é—´å‘¨æœŸåˆ›å»ºä¸“é—¨çš„è·å–è®¡åˆ’
        plan = {
            "1m": {
                "start": (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d"),
                "end": end_date,
            },
            "3m": {
                "start": (datetime.now() - timedelta(days=60)).strftime("%Y-%m-%d"),
                "end": end_date,
            },
            "5m": {
                "start": (datetime.now() - timedelta(days=90)).strftime("%Y-%m-%d"),
                "end": end_date,
            },
            "15m": {
                "start": (datetime.now() - timedelta(days=180)).strftime("%Y-%m-%d"),
                "end": end_date,
            },
            "30m": {
                "start": (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d"),
                "end": end_date,
            },
            "1h": {"start": start_date, "end": end_date},
            "4h": {"start": start_date, "end": end_date},
            "1d": {"start": start_date, "end": end_date},
        }

        results = []
        for tf, dates in plan.items():
            print(f"\nå¼€å§‹è·å– {symbol} {tf} æ•°æ®: {dates['start']} è‡³ {dates['end']}")
            file_path = self.fetch_data(symbol, tf, dates["start"], dates["end"])
            if file_path:
                results.append(file_path)

        return results


def run_step1_default(
    days: int = 200,
    exchange_name: str | None = None,
    symbol_exchange: str | None = None,
    market_type: str | None = None,
    base_tf: str | None = None,
    downloads_dir: str | None = None,
) -> None:
    """é»˜è®¤ä¸‹è½½æœ€è¿‘ days å¤©åˆ° main.io.downloads_dirï¼Œå¹¶æŒ‰ base_download å‘½åè¡¥é½èšåˆCSVã€‚"""
    from features_engineering.congfigs.config_loader import ConfigLoader

    loader = ConfigLoader()
    main_cfg = loader.load_main_config() or {}

    exchange_name = exchange_name or (main_cfg.get("exchange", {}) or {}).get("name", "binance")
    symbol_exchange = symbol_exchange or (main_cfg.get("symbol", {}) or {}).get("trading_pair_exchange", "ETH/USDT")
    # ğŸ”¥ ä» main_config è¯»å– trading_pair_std ç”¨äºæ–‡ä»¶å‘½å
    symbol_std = (main_cfg.get("symbol", {}) or {}).get("trading_pair_std", None)
    market_type = market_type or (main_cfg.get("symbol", {}) or {}).get("market_type", "swap")
    base_tf = base_tf or (main_cfg.get("timeframes", {}) or {}).get("base_download", "1m")
    _io = main_cfg.get("io", {}) or {}
    if downloads_dir:
        downloads_dir = os.path.abspath(downloads_dir)
    else:
        _base_dir = _io.get("base_dir") or os.path.join(os.path.expanduser("~"), "FinRL_bn", "data")
        downloads_dir = _io.get("downloads_dir") or f"{_base_dir}/rl_live/data_downloads"

    print(
        f"\nğŸš€ Step1 é»˜è®¤æ¨¡å¼: ä¸‹è½½æœ€è¿‘ {days} å¤© | {exchange_name} {symbol_exchange} ({market_type}) {base_tf}"
    )
    fetcher = EnhancedCCXTProcessor(exchange_name=exchange_name, market_type=market_type)
    # ğŸ”¥ é»˜è®¤æ¨¡å¼ï¼šä» main_config è¯»å– trading_pair_std æ§åˆ¶æ–‡ä»¶å
    fetcher.update_base_csv(
        symbol=symbol_exchange,
        base_tf=base_tf,
        output_dir=downloads_dir,
        days_if_missing=days,
        fill_missing=True,
        initial_start_str=None,
        symbol_std_override=symbol_std,  # ğŸ”¥ ä» main_config è¯»å–
    )


def run_step1_with_override(
    days: int = 280,
    symbol_std: str = "ETH_USDT",
    symbol_exchange: str = "ETH/USDT",
    exchange_name: str = "okx",
    market_type: str = "swap",
    base_tf: str = "1m",
    downloads_dir: str | None = None,
) -> None:
    """
    å¸¦å®Œæ•´å‚æ•°è¦†ç›–çš„ Step1 ä¸‹è½½ï¼ˆä¾›æ¯æ—¥è°ƒåº¦å™¨è°ƒç”¨ï¼‰
    
    Args:
        days: ä¸‹è½½æœ€è¿‘å¤šå°‘å¤©
        symbol_std: æ ‡å‡†åŒ–å¸ç§åï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰ï¼Œå¦‚ "ETH_USDT"
        symbol_exchange: äº¤æ˜“æ‰€æ ¼å¼å¸ç§åï¼Œå¦‚ "ETH/USDT"
        exchange_name: äº¤æ˜“æ‰€åç§°ï¼Œå¦‚ "okx" / "binance"
        market_type: å¸‚åœºç±»å‹ï¼Œå¦‚ "swap" / "spot"
        base_tf: åŸºç¡€æ—¶é—´å‘¨æœŸï¼Œå¦‚ "1m"
        downloads_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä» main_config è¯»å–ï¼‰
    """
    from features_engineering.congfigs.config_loader import ConfigLoader

    loader = ConfigLoader()
    main_cfg = loader.load_main_config() or {}

    # ç¡®å®šè¾“å‡ºç›®å½•
    if downloads_dir:
        downloads_dir = os.path.abspath(downloads_dir)
    else:
        _io = main_cfg.get("io", {}) or {}
        _base_dir = _io.get("base_dir") or os.path.join(os.path.expanduser("~"), "FinRL_bn", "data")
        downloads_dir = _io.get("downloads_dir") or f"{_base_dir}/rl_live/data_downloads"

    print(f"\nğŸš€ Step1 è¦†ç›–æ¨¡å¼: {symbol_std} ({symbol_exchange})")
    print(f"   äº¤æ˜“æ‰€: {exchange_name} | å¸‚åœº: {market_type} | å‘¨æœŸ: {base_tf} | å¤©æ•°: {days}")
    print(f"   è¾“å‡ºç›®å½•: {downloads_dir}")

    fetcher = EnhancedCCXTProcessor(exchange_name=exchange_name, market_type=market_type)
    
    # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ symbol_std_override æ§åˆ¶æ–‡ä»¶åï¼Œä¸ main_config å®Œå…¨è§£è€¦
    fetcher.update_base_csv(
        symbol=symbol_exchange,
        base_tf=base_tf,
        output_dir=downloads_dir,
        days_if_missing=days,
        fill_missing=True,
        initial_start_str=None,
        symbol_std_override=symbol_std,  # ğŸ”¥ å¤–æŒ‚ä¼ å…¥çš„æ ‡å‡†åŒ–åç§°
    )


# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼šé»˜è®¤ä¸‹è½½æœ€è¿‘200å¤©åŸºç¡€å‘¨æœŸå¹¶è¡¥é½å†™å›
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸ“¥ Step1 é»˜è®¤ä¸‹è½½ï¼ˆæœ€è¿‘200å¤©ï¼‰")
    print("=" * 60)
    run_step1_default(days=200)
