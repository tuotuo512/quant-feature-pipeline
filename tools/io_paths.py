from __future__ import annotations

import os
import re
from typing import Optional, Dict, Any, List
import pandas as pd


def read_df_auto(path: str) -> pd.DataFrame:
    if path.lower().endswith(".csv"):
        return pd.read_csv(path)
    if path.lower().endswith(".parquet"):
        return pd.read_parquet(path)
    base, _ = os.path.splitext(path)
    for cand in (base + ".parquet", base + ".csv"):
        if os.path.exists(cand):
            return read_df_auto(cand)
    raise FileNotFoundError(path)


def write_df_auto(df: pd.DataFrame, path: str) -> None:
    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
    if path.lower().endswith(".csv"):
        df.reset_index().to_csv(path, index=False)
        return
    if path.lower().endswith(".parquet"):
        df.to_parquet(path, index=True)
        return
    # é»˜è®¤parquet
    df.to_parquet(path + ".parquet", index=True)


def get_last_timestamp(path: str, fast: bool = True) -> pd.Timestamp | None:
    """
    è·å–æ–‡ä»¶çš„æœ€æ–°æ—¶é—´æˆ³

    Args:
        path: æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒcsv/parquetï¼Œè‡ªåŠ¨æŸ¥æ‰¾ï¼‰
        fast: True=ä»…è¯»å–æœ€åå‡ è¡Œï¼ˆå¿«é€Ÿï¼‰ï¼ŒFalse=è¯»å–å…¨æ–‡ä»¶ï¼ˆç²¾ç¡®ï¼‰

    Returns:
        æœ€æ–°æ—¶é—´æˆ³ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        # è§£æå®é™…è·¯å¾„
        actual_path = path
        if not os.path.exists(path):
            base, _ = os.path.splitext(path)
            for cand in (base + ".parquet", base + ".csv"):
                if os.path.exists(cand):
                    actual_path = cand
                    break

        if not os.path.exists(actual_path):
            return None

        # ğŸ”¥ å¿«é€Ÿæ¨¡å¼ï¼šä»…è¯»å–æœ€åNè¡Œ
        if fast:
            if actual_path.lower().endswith(".parquet"):
                # Parquetï¼šä½¿ç”¨pyarrowç›´æ¥è¯»å–æœ€å100è¡Œï¼ˆæœ€å¿«ï¼‰
                try:
                    import pyarrow.parquet as pq

                    # æ–¹æ³•1ï¼šè¯»å–metadataè·å–æ€»è¡Œæ•°
                    parquet_file = pq.ParquetFile(actual_path)
                    total_rows = parquet_file.metadata.num_rows

                    # åªè¯»æœ€å100è¡Œ
                    if total_rows > 100:
                        # ä½¿ç”¨pyarrowçš„åˆ‡ç‰‡è¯»å–ï¼ˆæ¯”pandaså¿«ï¼‰
                        table = parquet_file.read_row_groups([parquet_file.num_row_groups - 1])
                        df = table.to_pandas()
                        df = df.iloc[-100:]
                    else:
                        df = pd.read_parquet(actual_path)
                except Exception:
                    # å›é€€ï¼šç›´æ¥pandasè¯»å–ï¼ˆä¼šè¯»å…¨éƒ¨ï¼Œä½†ä¹Ÿå¤Ÿå¿«ï¼‰
                    df = pd.read_parquet(actual_path)
                    df = df.iloc[-100:] if len(df) > 100 else df
            else:
                # CSVï¼šä½¿ç”¨tailå‘½ä»¤ï¼ˆLinuxï¼‰æˆ–pandas
                try:
                    # æ–¹æ³•1ï¼šç³»ç»Ÿtailï¼ˆæœ€å¿«ï¼‰
                    import subprocess

                    result = subprocess.run(
                        ["tail", "-n", "100", actual_path], capture_output=True, text=True, timeout=2
                    )
                    if result.returncode == 0:
                        from io import StringIO

                        df = pd.read_csv(StringIO(result.stdout))
                    else:
                        raise Exception("tail failed")
                except Exception:
                    # æ–¹æ³•2ï¼špandasè¯»å–æœ€åNè¡Œ
                    df = pd.read_csv(actual_path)
                    df = df.iloc[-100:] if len(df) > 100 else df
        else:
            # å®Œæ•´æ¨¡å¼ï¼šè¯»å–å…¨æ–‡ä»¶
            df = read_df_auto(actual_path)

        # æŸ¥æ‰¾æ—¶é—´åˆ—
        ts_col = None
        for cand in ["timestamp", "time", "datetime", "ts"]:
            if cand in df.columns:
                ts_col = cand
                break

        if ts_col:
            # æ™ºèƒ½æ£€æµ‹ï¼šæ•´æ•°ç”¨æ¯«ç§’ï¼Œå­—ç¬¦ä¸²è‡ªåŠ¨æ¨æ–­
            if pd.api.types.is_integer_dtype(df[ts_col]):
                ts = pd.to_datetime(df[ts_col], unit="ms", errors="coerce")
            else:
                ts = pd.to_datetime(df[ts_col], errors="coerce")
            return ts.max()

        # å¦‚æœæœ‰ç´¢å¼•ä¸”æ˜¯æ—¶é—´ç±»å‹
        if isinstance(df.index, pd.DatetimeIndex):
            return df.index.max()

        # å°è¯•ç¬¬ä¸€åˆ—
        if len(df.columns) > 0:
            first_col = df.iloc[:, 0]
            if pd.api.types.is_integer_dtype(first_col):
                ts = pd.to_datetime(first_col, unit="ms", errors="coerce")
            else:
                ts = pd.to_datetime(first_col, errors="coerce")
            if ts.notna().any():
                return ts.max()

        return None
    except Exception:
        return None


def print_latest_timestamp(path: str, label: str = "ç›®å‰æœ€æ–°æ—¥æœŸ", fast: bool = True) -> None:
    """
    æ‰“å°æ–‡ä»¶çš„æœ€æ–°æ—¶é—´æˆ³ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰

    Args:
        path: æ–‡ä»¶è·¯å¾„
        label: æ‰“å°æ ‡ç­¾
        fast: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼ˆä»…è¯»æœ€å100è¡Œï¼‰
    """
    try:
        latest_ts = get_last_timestamp(path, fast=fast)
        if isinstance(latest_ts, pd.Timestamp) and not pd.isna(latest_ts):
            print(f"ğŸ“… {label}ï¼š{latest_ts.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}")
    except Exception:
        pass


def print_latest_timestamp_from_df(df: pd.DataFrame, label: str = "ç›®å‰æœ€æ–°æ—¥æœŸ") -> None:
    """
    ä»DataFrameæ‰“å°æœ€æ–°æ—¶é—´æˆ³ï¼ˆå·²åŠ è½½åˆ°å†…å­˜çš„æƒ…å†µï¼‰

    Args:
        df: DataFrameï¼ˆåº”è¯¥æœ‰timestampåˆ—æˆ–DatetimeIndexï¼‰
        label: æ‰“å°æ ‡ç­¾
    """
    try:
        latest_ts = None

        # æ–¹æ³•1ï¼šä»ç´¢å¼•è·å–
        if isinstance(df.index, pd.DatetimeIndex):
            latest_ts = df.index.max()

        # æ–¹æ³•2ï¼šä»timestampåˆ—è·å–
        need_fallback = False
        if latest_ts is None:
            need_fallback = True
        elif isinstance(latest_ts, pd.Timestamp) and pd.isna(latest_ts):
            need_fallback = True
        if need_fallback:
            for ts_col in ["timestamp", "time", "datetime", "ts"]:
                if ts_col in df.columns:
                    ts = pd.to_datetime(df[ts_col], errors="coerce")
                    latest_ts = ts.max()
                    break

        if isinstance(latest_ts, pd.Timestamp) and not pd.isna(latest_ts):
            print(f"ğŸ“… {label}ï¼š{latest_ts.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}")
    except Exception:
        pass


# =========================
# ç»Ÿä¸€ IO ç®¡ç†å™¨ï¼ˆé›†ä¸­å¼è·¯å¾„ä¸è¯»å†™ï¼‰
# =========================


class IOManager:
    """
    ç»Ÿä¸€çš„ IO ç®¡ç†å™¨ï¼šåŸºäº main_config.yaml çš„ io é…ç½®ä¸æ¨¡æ¿ï¼Œæä¾›æ ‡å‡†è·¯å¾„ä¸è¯»å†™æ¥å£ã€‚
    çº¦æŸï¼š
    - Step1 å¼ºåˆ¶ CSV
    - Step2-4 é»˜è®¤ Parquetï¼ˆæˆ–éµä» io.output_format: csv|parquet|bothï¼‰
    - Step5 å¼ºåˆ¶ NPZï¼ˆæœ¬ç±»ä¸è´Ÿè´£å†™å…¥ NPZï¼Œåªè´Ÿè´£è·¯å¾„ï¼‰
    """

    def __init__(self, config: Dict[str, Any]):
        self.cfg = config or {}
        self.io_cfg = self.cfg.get("io", {}) or {}
        self.patterns = self.io_cfg.get("filename_patterns") or {}

        # ç›®å½•
        self.base_dir = self._abspath(
            self.io_cfg.get("base_dir") or os.path.join(os.path.expanduser("~"), "FinRL_bn", "data")
        )
        self.downloads_dir = self._abspath(
            self.io_cfg.get("downloads_dir") or os.path.join(self.base_dir, "rl_live", "data_downloads")
        )
        self.kline_dir = self._abspath(
            self.io_cfg.get("kline_dir") or os.path.join(self.base_dir, "rl_live", "kline")
        )
        self.indicators_dir = self._abspath(
            self.io_cfg.get("indicators_dir") or os.path.join(self.base_dir, "rl_live", "ind")
        )
        self.merged_dir = self._abspath(
            self.io_cfg.get("merged_dir") or os.path.join(self.base_dir, "rl_live", "merged")
        )
        self.rl_ready_dir = self._abspath(
            self.io_cfg.get("rl_ready_dir") or os.path.join(self.base_dir, "rl_live", "data_ready")
        )

        # å…¶ä»–é…ç½®
        self.output_format = (
            str(self.io_cfg.get("output_format", "parquet")).lower().strip()
        )  # ä»…å½±å“ Step2-4
        self.overwrite = bool(self.io_cfg.get("overwrite", False))

        # æ¸²æŸ“ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæ¨¡æ¿ {a.b}ï¼‰
        self.context = self._build_context()

    # ---------- å…¬å…± API ----------
    def path_for(self, kind: str, *, timeframe: Optional[str] = None, roll: bool = False) -> str:
        # ğŸ”¥ å…³é”®ï¼šè‹¥æœªæä¾›æˆ–ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œåˆ™å¯¹éœ€è¦å‘¨æœŸçš„ç§ç±»è‡ªåŠ¨å›é€€åˆ° main_config çš„ base_download
        if (timeframe is None or (isinstance(timeframe, str) and not timeframe.strip())) and kind in (
            "download",
            "kline",
            "indicator",
        ):
            base_tf = (self.cfg.get("timeframes", {}) or {}).get("base_download") or "1m"
            timeframe = str(base_tf).strip()
        # ç¡®ä¿ timeframe æ˜¯å­—ç¬¦ä¸²ä¸”éç©º
        if timeframe is None:
            timeframe = "1m"
        timeframe = str(timeframe).strip() or "1m"
        dir_path, template = self._dir_and_template(kind, roll=roll)
        name = self._render_template(template, timeframe=timeframe)
        return self._abspath(os.path.join(dir_path, name))

    def resolve_existing(self, path: str, prefer: Optional[List[str]] = None) -> Optional[str]:
        """
        æ ¹æ®åå¥½é¡ºåºï¼ˆé»˜è®¤ ['parquet','csv']ï¼‰è§£æå®é™…å­˜åœ¨çš„æ–‡ä»¶ã€‚
        è‹¥ path å­˜åœ¨åˆ™ç›´æ¥è¿”å›ï¼›å¦åˆ™å°è¯•äº’æ¢æ‰©å±•åæˆ–é™„åŠ æ‰©å±•åã€‚
        """
        prefer = prefer or ["parquet", "csv"]
        ap = self._abspath(path)
        if os.path.exists(ap):
            return ap
        root, ext = os.path.splitext(ap)
        cand_exts: List[str] = []
        if ext.lower() in (".csv", ".parquet"):
            other = ".parquet" if ext.lower() == ".csv" else ".csv"
            cand_exts = [other]
        else:
            cand_exts = ["." + e for e in prefer]
        for e in cand_exts:
            cand = root + e
            if os.path.exists(cand):
                return cand
        return None

    def read_table(self, kind: str, *, timeframe: Optional[str] = None, roll: bool = False) -> pd.DataFrame:
        """ç»Ÿä¸€è¯»å–è¡¨æ ¼ï¼ˆè‡ªåŠ¨å…œåº• .parquet/.csvï¼‰ã€‚"""
        path = self.path_for(kind, timeframe=timeframe, roll=roll)
        actual = self.resolve_existing(path)
        # å…¼å®¹å†å²è·¯å¾„ï¼šindicator/kline å­˜åœ¨å­ç›®å½• <timeframe>/ çš„æƒ…å†µ
        if not actual and timeframe:
            name = os.path.basename(path)
            tf_variants = [str(timeframe), str(timeframe).lower(), str(timeframe).upper()]
            if kind == "indicator":
                for tf_dir in tf_variants:
                    alt = self._abspath(os.path.join(self.indicators_dir, tf_dir, name))
                    actual = self.resolve_existing(alt)
                    if actual:
                        break
            elif kind == "kline":
                for tf_dir in tf_variants:
                    alt = self._abspath(os.path.join(self.kline_dir, tf_dir, name))
                    actual = self.resolve_existing(alt)
                    if actual:
                        break
        if not actual:
            raise FileNotFoundError(path)
        return read_df_auto(actual)

    def write_table(
        self, kind: str, df: pd.DataFrame, *, timeframe: Optional[str] = None, roll: bool = False
    ) -> List[str]:
        """
        ç»Ÿä¸€å†™è¡¨ï¼šè¿”å›å®é™…å†™å…¥çš„æ–‡ä»¶åˆ—è¡¨ã€‚
        - Step1(download): å¼ºåˆ¶ CSV
        - Step2-4(kline/indicator/merged): éµä» io.output_format
        - Step5: æœ¬å‡½æ•°ä¸è´Ÿè´£
        """
        path = self.path_for(kind, timeframe=timeframe, roll=roll)
        root, ext = os.path.splitext(path)
        written: List[str] = []

        if kind == "download":
            out = root + ".csv"
            self._ensure_dir(out)
            df.reset_index().to_csv(out, index=False)
            written.append(out)
            return written

        if kind in ("kline", "indicator", "merged"):
            fmt = self.output_format
            if fmt == "parquet":
                out = root + ".parquet"
                self._ensure_dir(out)
                df.to_parquet(out, index=True)
                written.append(out)
            elif fmt == "csv":
                out = root + ".csv"
                self._ensure_dir(out)
                df.reset_index().to_csv(out, index=False)
                written.append(out)
            elif fmt == "both":
                out_pq = root + ".parquet"
                out_csv = root + ".csv"
                self._ensure_dir(out_pq)
                self._ensure_dir(out_csv)
                df.to_parquet(out_pq, index=True)
                df.reset_index().to_csv(out_csv, index=False)
                written.extend([out_pq, out_csv])
            else:
                # é»˜è®¤ parquet
                out = root + ".parquet"
                self._ensure_dir(out)
                df.to_parquet(out, index=True)
                written.append(out)
            return written

        # å…¶ä»– kindï¼ˆå¦‚ rl_features/rl_labelsï¼‰ä»…æä¾›è·¯å¾„ï¼Œä¸è´Ÿè´£å†™å…¥
        return written

    # ---------- å†…éƒ¨å·¥å…· ----------
    def _dir_and_template(self, kind: str, *, roll: bool) -> tuple[str, str]:
        # ç›®å½•é€‰æ‹©
        dir_map = {
            "download": self.downloads_dir,
            "kline": self.kline_dir,
            "indicator": self.indicators_dir,
            "merged": self.merged_dir,
            "rl": self.rl_ready_dir,
            "rl_features": self.rl_ready_dir,
            "rl_labels": self.rl_ready_dir,
        }
        directory = dir_map.get(kind, self.base_dir)

        # æ¨¡æ¿é€‰æ‹©
        # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„æ¨¡æ¿ï¼Œå¦åˆ™é™çº§ä¸ºåˆç†é»˜è®¤
        default_patterns = {
            "download": "{symbol.trading_pair_std}_{market_type}_{timeframe}.csv",
            "kline": "{symbol.trading_pair_std}_{timeframe}.parquet",
            "kline_roll": "{symbol.trading_pair_std}_{timeframe}_roll.parquet",
            "indicator": "{symbol.trading_pair_std}_{timeframe}_indicators.parquet",
            "merged": "{symbol.trading_pair_std}_$timeframe$_merged.parquet",
            "rl": "{symbol.trading_pair_std}_rl.npz",
            "rl_features": "{symbol.trading_pair_std}_$timeframe$_rl_features.npz",
            "rl_labels": "{symbol.trading_pair_std}_$timeframe$_rl_labels.npz",
        }

        if kind == "kline" and roll:
            template_key = "kline_roll"
        else:
            template_key = kind if kind in default_patterns else "merged"

        template = self.patterns.get(template_key, default_patterns[template_key])
        return directory, template

    def _render_template(self, template: str, *, timeframe: Optional[str]) -> str:
        """
        æ¸²æŸ“æ¨¡æ¿å ä½ç¬¦ï¼š
        - {a.b} å•èŠ±æ‹¬å·ï¼šç”± ConfigLoader åœ¨åŠ è½½é…ç½®æ—¶æ›¿æ¢ï¼ˆé™æ€é…ç½®å€¼ï¼‰
        - $xxx$ ç¾å…ƒç¬¦å·ï¼šç”± IOManager è¿è¡Œæ—¶æ›¿æ¢ï¼ˆåŠ¨æ€è¿è¡Œæ—¶å‚æ•°ï¼Œå¦‚ timeframeï¼‰
        """
        # é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆè¿è¡Œæ—¶ï¼‰
        runtime_ctx = dict(self.context)
        # ğŸ”¥ å…³é”®ï¼štimeframe å¿…é¡»æœ‰å€¼
        if timeframe is None or (isinstance(timeframe, str) and not timeframe.strip()):
            timeframe = runtime_ctx.get("timeframes.base_download", "1m")
        # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ä¸”éç©º
        timeframe = str(timeframe).strip() or "1m"
        runtime_ctx["timeframe"] = timeframe

        def deep_get(d: Dict[str, Any], dotted: str) -> Any:
            cur: Any = d
            for part in dotted.split("."):
                if isinstance(cur, dict) and part in cur:
                    cur = cur[part]
                else:
                    return None
            return cur

        # ğŸ”¥ å…³é”®ï¼šæ›¿æ¢ $xxx$ è¿è¡Œæ—¶å‚æ•°
        def repl_runtime(m: re.Match) -> str:
            key = m.group(1)
            # è¿è¡Œæ—¶å‚æ•°ä¼˜å…ˆä» runtime_ctx è·å–
            if key in runtime_ctx:
                val = runtime_ctx[key]
            else:
                val = deep_get(self.cfg, key)
            if val is None:
                raise KeyError(f"è¿è¡Œæ—¶å‚æ•° '${key}$' æœªæ‰¾åˆ°ï¼Œæ¨¡æ¿ï¼š{template}")
            return str(val)

        # æ›¿æ¢ $xxx$ è¿è¡Œæ—¶å‚æ•°
        result = re.sub(r"\$([\w\.]+)\$", repl_runtime, template)

        # å…œåº•ï¼šæ›¿æ¢å•èŠ±æ‹¬å·ï¼ˆç†è®ºä¸Šå·²è¢« ConfigLoader æ›¿æ¢ï¼‰
        def repl_single(m: re.Match) -> str:
            key = m.group(1)
            if key in runtime_ctx:
                val = runtime_ctx[key]
            else:
                val = deep_get(self.cfg, key)
            if val is None:
                return ""
            return str(val)

        result = re.sub(r"\{([\w\.]+)\}", repl_single, result)
        return result

    def _build_context(self) -> Dict[str, Any]:
        ctx: Dict[str, Any] = {}
        # å¸¸ç”¨å­—æ®µï¼ˆé¿å…æ¨¡æ¿ç¼ºå¤±æ—¶å…œåº•ï¼‰
        symbol = self.cfg.get("symbol", {}) or {}
        ctx["symbol.trading_pair_std"] = symbol.get("trading_pair_std", "ETH_USDT")
        ctx["symbol.trading_pair_exchange"] = symbol.get("trading_pair_exchange", "ETH/USDT")
        # ConfigLoader.load_main_config å·²å°† market_type æå‡åˆ°é¡¶å±‚å¹¶å¤§å†™
        ctx["market_type"] = self.cfg.get("market_type", symbol.get("market_type", "SWAP")).upper()
        # timeframes.base_download
        tf = self.cfg.get("timeframes", {}) or {}
        ctx["timeframes.base_download"] = tf.get("base_download", "1m")
        return ctx

    def get_min_resample_timeframe(self) -> str:
        """è·å– resample_targets ä¸­æœ€å°çš„æ—¶é—´å‘¨æœŸï¼ˆå•ä½æŒ‰ m/h/d è§£æï¼‰ã€‚"""
        tf_cfg = self.cfg.get("timeframes", {}) or {}
        targets = tf_cfg.get("resample_targets") or []
        base_download = tf_cfg.get("base_download", "1m")
        if not targets:
            return base_download

        def _to_minutes(s: str) -> int:
            try:
                s2 = str(s).strip().lower()
                m = re.match(r"^(\d+)([mhd])$", s2)
                if not m:
                    return 10**9
                val = int(m.group(1))
                unit = m.group(2)
                mult = 1 if unit == "m" else (60 if unit == "h" else 1440)
                return val * mult
            except Exception:
                return 10**9

        try:
            return sorted(targets, key=_to_minutes)[0]
        except Exception:
            return base_download

    @staticmethod
    def _abspath(p: str) -> str:
        return os.path.abspath(p)

    @staticmethod
    def _ensure_dir(path: str) -> None:
        d = os.path.dirname(os.path.abspath(path))
        if d:
            os.makedirs(d, exist_ok=True)
