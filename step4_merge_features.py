#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step4: ç‰¹å¾èžåˆå™¨ï¼ˆå®Œå…¨é…ç½®é©±åŠ¨ç‰ˆï¼‰

âœ¨ ç‰¹æ€§ï¼š
  - å®Œå…¨ç”± main_config.yaml + step4_merge.yaml é©±åŠ¨
  - ä»¥åŸºç¡€å‘¨æœŸä¸ºæ—¶é—´è½´ï¼Œå¯¹é½å¤šå‘¨æœŸæŒ‡æ ‡
  - ä½¿ç”¨ merge_asof backward å¯¹é½ç­–ç•¥
  - æ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤

ðŸ“‹ ç”¨æ³•ï¼š
  python step4_merge_features.py
  python step4_merge_features.py --start 2024-01-01 --end 2024-12-31

ðŸ”§ é…ç½®ï¼š
  - å…¨å±€é…ç½®: main_config.yaml
  - èžåˆç­–ç•¥: step4_merge.yaml
"""

from __future__ import annotations

import os
import sys
import argparse
import re

try:
    import pandas as pd
except ImportError:
    print("âŒ å¯¼å…¥ pandas å¤±è´¥ï¼Œè¯·è¿è¡Œ: pip install pandas")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
_PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if _PROJECT_ROOT not in sys.path:
    sys.path.insert(0, _PROJECT_ROOT)

try:
    from features_engineering.congfigs.config_loader import ConfigLoader
except Exception as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

# å·¥å…·ï¼ˆè¯»å†™/å¢žé‡ï¼‰
from features_engineering.tools.io_paths import (
    read_df_auto,
    IOManager,
    print_latest_timestamp_from_df,
)
from features_engineering.tools.incremental import safe_concat_dedup


def ensure_dir(dir_path: str) -> str:
    """ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶è¿”å›žå…¶ç»å¯¹è·¯å¾„"""
    if not dir_path:
        return dir_path
    abs_dir = os.path.abspath(dir_path)
    os.makedirs(abs_dir, exist_ok=True)
    return abs_dir


def parse_args() -> argparse.Namespace:
    """è§£æžå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Step4: ç‰¹å¾èžåˆå™¨ï¼ˆå®Œå…¨é…ç½®é©±åŠ¨ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤é…ç½®
  python step4_merge_features.py

  # æŒ‡å®šæ—¶é—´èŒƒå›´
  python step4_merge_features.py --start 2024-01-01 --end 2024-12-31

  # è¦†ç›–è¾“å‡ºæ ¼å¼
  python step4_merge_features.py --output_format parquet
        """,
    )
    parser.add_argument("--start", type=str, default=None, help="èµ·å§‹æ—¶é—´(å¯é€‰)ï¼Œå¦‚ 2024-01-01")
    parser.add_argument("--end", type=str, default=None, help="ç»“æŸæ—¶é—´(å¯é€‰)ï¼Œå¦‚ 2024-12-31")
    parser.add_argument(
        "--output_format",
        type=str,
        default=None,
        choices=["csv", "parquet", "both"],
        help="è¦†ç›–è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤ä»Ž main_config.yaml è¯»å–ï¼‰",
    )
    return parser.parse_args()


def read_base_1m(base_file: str, start: str | None, end: str | None) -> pd.DataFrame:
    if not os.path.exists(base_file):
        raise FileNotFoundError(f"1m åŸºç¡€CSVä¸å­˜åœ¨: {base_file}")

    # å°è¯•ä»¥ç¬¬ä¸€åˆ—ä¸ºæ—¶é—´ç´¢å¼•è¯»å–
    try:
        df = pd.read_csv(base_file, parse_dates=[0], index_col=0)
        if df.index.name is None:
            df.index.name = "timestamp"
    except Exception:
        df = pd.read_csv(base_file)
        ts_col = None
        for cand in ["timestamp", "time", "datetime", "ts"]:
            if cand in df.columns:
                ts_col = cand
                break
        if ts_col is None:
            raise ValueError("CSV ä¸­æœªæ‰¾åˆ°æ—¶é—´åˆ—ï¼ˆtimestamp/time/datetime/tsï¼‰")
        df[ts_col] = pd.to_datetime(df[ts_col], errors="coerce")
        df = df.set_index(ts_col)
        df.index.name = "timestamp"

    # åªä¿ç•™æ ‡å‡†åˆ—
    keep_cols = [c for c in ["open", "high", "low", "close", "volume"] if c in df.columns]
    df = df[keep_cols]
    df = df[~df.index.duplicated(keep="last")].sort_index()

    # åˆ‡ç‰‡
    if start:
        df = df[df.index >= pd.to_datetime(start)]
    if end:
        df = df[df.index <= pd.to_datetime(end)]
    if df.empty:
        raise ValueError("ç­›é€‰åŽçš„1mæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´æˆ–è¾“å…¥æ–‡ä»¶å†…å®¹")
    return df


def _tf_to_minutes(tf: str) -> int:
    """å°†æ—¶é—´å‘¨æœŸå­—ç¬¦ä¸²ï¼ˆå¦‚ '1m','3m','2h','1d','1w'ï¼‰è½¬æ¢ä¸ºåˆ†é’Ÿæ•°ï¼Œç”¨äºŽæ¯”è¾ƒå¤§å°ã€‚"""
    try:
        m = re.fullmatch(r"(\d+)([mhdw])", str(tf).strip().lower())
        if not m:
            return 1_000_000  # æœªè¯†åˆ«çš„æ”¾åˆ°æžå¤§ï¼Œé¿å…è¢«é€‰ä¸ºæœ€å°
        n = int(m.group(1))
        unit = m.group(2)
        factor = {"m": 1, "h": 60, "d": 1440, "w": 10080}.get(unit, 1)
        return n * factor
    except Exception:
        return 1_000_000


def read_kline_for_tf(
    kline_root: str, symbol: str, tf: str, start: str | None, end: str | None
) -> pd.DataFrame:
    """è¯»å–å·²é‡é‡‡æ ·çš„Kçº¿ä½œä¸ºåŸºå‡†æ—¶é—´è½´ï¼ˆæ¥è‡ª rl_live/klineï¼‰ã€‚ä¼˜å…ˆParquetï¼Œå›žé€€CSVã€‚"""
    base = os.path.abspath(kline_root)
    p_parquet = os.path.join(base, f"{symbol}_{tf}.parquet")
    p_csv = os.path.join(base, f"{symbol}_{tf}.csv")

    df = None
    if os.path.exists(p_parquet):
        df = pd.read_parquet(p_parquet)
    elif os.path.exists(p_csv):
        df = pd.read_csv(p_csv)
    else:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°Kçº¿æ–‡ä»¶: {p_parquet} æˆ– {p_csv}")

    # è§£æžæ—¶é—´ç´¢å¼•
    if isinstance(df, pd.DataFrame):
        if "timestamp" in df.columns:
            df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
            df = df.set_index("timestamp")
        elif not isinstance(df.index, pd.DatetimeIndex):
            try:
                df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0], errors="coerce")
                df = df.set_index(df.columns[0])
            except Exception:
                raise ValueError("Kçº¿æ–‡ä»¶æ— æ³•è¯†åˆ«æ—¶é—´åˆ—")
        df.index.name = "timestamp"
        # ä»…ä¿ç•™æ ‡å‡†OHLCVåˆ—ï¼ˆå­˜åœ¨åˆ™ä¿ç•™ï¼‰
        keep_cols = [c for c in ["open", "high", "low", "close", "volume"] if c in df.columns]
        if keep_cols:
            df = df[keep_cols]
        df = df[~df.index.duplicated(keep="last")].sort_index()
        # åˆ‡ç‰‡
        if start:
            df = df[df.index >= pd.to_datetime(start)]
        if end:
            df = df[df.index <= pd.to_datetime(end)]
        if df.empty:
            raise ValueError("ç­›é€‰åŽçš„åŸºå‡†Kçº¿æ•°æ®ä¸ºç©º")
        return df
    else:
        raise ValueError("è¯»å–Kçº¿å¤±è´¥ï¼šæ•°æ®æ ¼å¼å¼‚å¸¸")


def read_ind_for_tf(
    ind_root: str,
    symbol: str,
    tf: str,
    start: str | None,
    end: str | None,
    preferred_fmt: str | None = None,
    indicator_pattern: str | None = None,
) -> pd.DataFrame:
    # ç›®å½•é‡‡ç”¨æ ¹ç›®å½•ï¼Œä¸è¿›å…¥å­ç›®å½•ï¼ˆéµå¾ª main_config.yaml æ¨¡æ¿ï¼‰
    base = os.path.abspath(ind_root)

    # ä¼˜å…ˆä½¿ç”¨ main_config.yaml ä¸­çš„æ¨¡æ¿ï¼šio.filename_patterns.indicator
    p_from_pattern_parquet = None
    p_from_pattern_csv = None
    if indicator_pattern:
        # å ä½æ›¿æ¢ï¼š{symbol.trading_pair_std}, $timeframe$
        fname_pat = indicator_pattern
        try:
            fname_pat = fname_pat.replace("{symbol.trading_pair_std}", str(symbol))
            fname_pat = fname_pat.replace("$timeframe$", str(tf))
        except Exception:
            pass
        p_from_pattern_parquet = os.path.join(base, fname_pat)
        # è‹¥æ¨¡æ¿æœªæŒ‡æ˜Žæ‰©å±•åï¼Œå°è¯•ä¸¤ç§
        if not os.path.splitext(fname_pat)[1]:
            p_from_pattern_parquet = os.path.join(base, fname_pat + ".parquet")
            p_from_pattern_csv = os.path.join(base, fname_pat + ".csv")
        else:
            # åå‘æŽ¨å¯¼å¦ä¸€ç§æ‰©å±•ä»¥ä½œå›žé€€
            ext = os.path.splitext(fname_pat)[1].lower()
            if ext == ".parquet":
                p_from_pattern_csv = os.path.join(base, os.path.splitext(fname_pat)[0] + ".csv")
            elif ext == ".csv":
                p_from_pattern_parquet = os.path.join(base, os.path.splitext(fname_pat)[0] + ".parquet")

    # é»˜è®¤å‘½åï¼ˆå‘åŽå…¼å®¹ï¼‰
    fname_ind = f"{symbol}_{tf}_indicators"
    fname_old = f"{symbol}_{tf}_ind"
    p_parquet = os.path.join(base, f"{fname_ind}.parquet")
    p_csv = os.path.join(base, f"{fname_ind}.csv")

    # å¦‚æžœæ–°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æ—§æ–‡ä»¶å
    if not os.path.exists(p_parquet) and not os.path.exists(p_csv):
        # å…¼å®¹æ—§å
        candidates = [
            os.path.join(base, f"{fname_old}.parquet"),
            os.path.join(base, f"{fname_old}.csv"),
            os.path.join(base, f"{symbol}_{tf}_fixed.parquet"),
            os.path.join(base, f"{symbol}_{tf}_fixed.csv"),
            os.path.join(base, f"{symbol}_{tf}_roll.parquet"),
            os.path.join(base, f"{symbol}_{tf}_roll.csv"),
        ]
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå­˜åœ¨çš„
        p_parquet = next((c for c in candidates if c.endswith(".parquet") and os.path.exists(c)), p_parquet)
        p_csv = next((c for c in candidates if c.endswith(".csv") and os.path.exists(c)), p_csv)

    # æŒ‰é¦–é€‰æ ¼å¼è¯»å–ï¼›è‹¥ä¸å­˜åœ¨åˆ™å›žé€€ï¼ˆä¼˜å…ˆæ¨¡æ¿è·¯å¾„ï¼‰
    fmt = (preferred_fmt or "csv").lower()
    fmt = "csv" if fmt not in ("csv", "parquet", "both") else fmt
    paths_try: list[tuple[str, str]]
    if fmt == "parquet":
        paths_try = [
            ("parquet", p_from_pattern_parquet or ""),
            ("csv", p_from_pattern_csv or ""),
            ("parquet", p_parquet),
            ("csv", p_csv),
        ]
    elif fmt == "csv":
        paths_try = [
            ("csv", p_from_pattern_csv or ""),
            ("parquet", p_from_pattern_parquet or ""),
            ("csv", p_csv),
            ("parquet", p_parquet),
        ]
    else:  # both
        paths_try = [
            ("csv", p_from_pattern_csv or ""),
            ("parquet", p_from_pattern_parquet or ""),
            ("csv", p_csv),
            ("parquet", p_parquet),
        ]

    df = None
    for kind, path in paths_try:
        if path and os.path.exists(path):
            if kind == "csv":
                df = pd.read_csv(path)
            else:
                df = pd.read_parquet(path)
            break
    if df is None:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æŒ‡æ ‡æ–‡ä»¶: {p_parquet} æˆ– {p_csv}")

    # è§£æžæ—¶é—´ç´¢å¼•
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
        df = df.set_index("timestamp")
    elif isinstance(df.index, pd.DatetimeIndex):
        pass
    else:
        try:
            df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0], errors="coerce")
            df = df.set_index(df.columns[0])
        except Exception:
            raise ValueError("æ— æ³•è¯†åˆ«æ—¶é—´åˆ—ï¼Œè¯·æ£€æŸ¥æŒ‡æ ‡æ–‡ä»¶")

    df.index.name = "timestamp"
    df = df[~df.index.duplicated(keep="last")].sort_index()
    if start:
        df = df[df.index >= pd.to_datetime(start)]
    if end:
        df = df[df.index <= pd.to_datetime(end)]
    if df.empty:
        raise ValueError(f"æŒ‡æ ‡{tf}ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´æˆ–è¾“å…¥æ–‡ä»¶")

    # ðŸ”§ æŽ’é™¤OHLCVåˆ—ï¼Œé¿å…ä¸ŽåŸºç¡€Kçº¿é‡å¤ï¼ˆé‡è¦ï¼ï¼‰
    ohlcv_cols = ["open", "high", "low", "close", "volume"]
    cols_to_drop = [c for c in ohlcv_cols if c in df.columns]
    if cols_to_drop:
        df = df.drop(columns=cols_to_drop)

    df = _standardize_indicator_columns(df)
    return df


def _standardize_indicator_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ç»Ÿä¸€æŒ‡æ ‡åˆ—å‘½åï¼Œå°† rsi14 â†’ rsiã€atr14_pct â†’ atr_pct ç­‰ã€‚"""
    renamed = df.copy()
    rename_map: dict[str, str] = {}
    used_targets = set(renamed.columns)
    for col in list(renamed.columns):
        if not isinstance(col, str):
            continue
        name = col.strip()
        target = None
        if re.fullmatch(r"rsi\d+", name):
            target = "rsi"
        elif re.fullmatch(r"atr\d+_pct", name):
            target = "atr_pct"
        elif re.fullmatch(r"atr\d+", name):
            target = "atr"
        elif re.fullmatch(r"macd_hist(?:ogram)?", name):
            target = "macd_histogram"
        if target and target not in used_targets and target not in rename_map.values():
            rename_map[col] = target
            used_targets.add(target)
    if rename_map:
        renamed = renamed.rename(columns=rename_map)
    return renamed


def prefix_columns(df: pd.DataFrame, tf: str) -> pd.DataFrame:
    """ä¸ºDataFrameçš„æ‰€æœ‰åˆ—æ·»åŠ å‘¨æœŸå‰ç¼€"""
    renamed = df.copy()
    renamed.columns = [f"{tf}_" + str(c) for c in renamed.columns]
    return renamed


def asof_merge_on_1m(base_1m: pd.DataFrame, tf_to_df: dict[str, pd.DataFrame]) -> pd.DataFrame:
    merged = base_1m.reset_index().sort_values("timestamp").copy()
    for tf, df in tf_to_df.items():
        df_pref = prefix_columns(df, tf).reset_index().sort_values("timestamp")
        merged = pd.merge_asof(
            merged,
            df_pref,
            on="timestamp",
            direction="backward",
            allow_exact_matches=True,
        )
    merged = merged.set_index("timestamp").sort_index()
    return merged


def save_output(df: pd.DataFrame, output_file: str, fmt: str):
    ensure_dir(os.path.dirname(os.path.abspath(output_file)))
    # åŸºç¡€QCï¼šåŽ»é‡+æŽ’åº+NaNè½»åº¦å¤„ç†ï¼ˆæ›´æ·±å…¥çš„QCåœ¨Step5ï¼‰
    if isinstance(df.index, pd.DatetimeIndex):
        df = df[~df.index.duplicated(keep="last")].sort_index()
    # å†™CSVä¼˜å…ˆï¼Œä¾¿äºŽæ£€æŸ¥
    if fmt in ("csv", "both"):
        if output_file.endswith(".csv"):
            c = output_file
        else:
            c = os.path.splitext(output_file)[0] + ".csv"
        df.reset_index().to_csv(c, index=False)
        print(f"âœ… å†™å…¥CSV: {c}")
    if fmt in ("parquet", "both"):
        if output_file.endswith(".parquet"):
            p = output_file
        else:
            p = os.path.splitext(output_file)[0] + ".parquet"
        try:
            df.to_parquet(p, index=True)
            print(f"âœ… å†™å…¥Parquet: {p}")
        except Exception as e:
            print(f"âš ï¸ Parquetå†™å…¥å¤±è´¥(å·²å¿½ç•¥): {e}")


def execute_step4(
    cfg: dict,
    start: str | None = None,
    end: str | None = None,
    output_format: str | None = None,
    *,
    verbose: bool = True,
) -> dict:
    """æ‰§è¡Œ Step4 ç‰¹å¾èžåˆé€»è¾‘ï¼Œä¾›è„šæœ¬ä¸Žç»Ÿä¸€æµæ°´çº¿å¤ç”¨ã€‚"""
    if not cfg:
        raise ValueError("é…ç½®ä¸èƒ½ä¸ºç©º")

    log = print if verbose else (lambda *args, **kwargs: None)

    log("ðŸš€ Step4 ç‰¹å¾èžåˆå¯åŠ¨ï¼ˆå®Œå…¨é…ç½®é©±åŠ¨ï¼‰\n")

    symbol = cfg.get("symbol", {}).get("trading_pair_std", "ETH_USDT")
    market_type = cfg.get("symbol", {}).get("market_type", "swap")

    timeframes_cfg = cfg.get("timeframes", {})
    base_download = timeframes_cfg.get("base_download", "1m")
    timeframes = timeframes_cfg.get("resample_targets", ["3m", "15m", "30m", "2h"])
    variant = str(timeframes_cfg.get("variant", "")).strip().lower()
    source_mode = cfg.get("rl_build", {}).get("source_mode", "fixed")

    io_cfg = cfg.get("io", {})
    io = IOManager(cfg)
    base_dir = io.base_dir
    downloads_dir = io.downloads_dir
    kline_dir = io.kline_dir
    ind_dir = io.indicators_dir
    merged_dir = io.merged_dir

    output_fmt = output_format or io_cfg.get("output_format", "csv")
    io_overwrite = bool(io_cfg.get("overwrite", False))

    merge_cfg = cfg.get("merge", {})
    include_base_ohlcv = merge_cfg.get("include_base_ohlcv", True)
    align_direction = merge_cfg.get("align_direction", "backward")
    allow_exact_match = merge_cfg.get("allow_exact_match", True)
    add_prefix = merge_cfg.get("add_timeframe_prefix", True)

    try:
        base_axis_tf = sorted(timeframes, key=_tf_to_minutes)[0] if timeframes else base_download
    except Exception:
        base_axis_tf = base_download

    if str(base_axis_tf).lower() == str(base_download).lower():
        base_kline_path = io.path_for("download", timeframe=base_download)
    else:
        base_kline_path = io.path_for("kline", timeframe=base_axis_tf)
    base_filename = os.path.basename(base_kline_path)

    output_filename = f"{symbol}_{base_axis_tf}_merged.{output_fmt}"
    output_file = os.path.join(merged_dir, output_filename)

    log("ðŸ“‹ é…ç½®æ‘˜è¦:")
    log(f"   äº¤æ˜“å¯¹: {symbol} ({market_type.upper()})")
    log(f"   åŸºç¡€å‘¨æœŸ(base_download): {base_download}")
    log(f"   ä¸»è½´å‘¨æœŸ(base_axis): {base_axis_tf}")
    log(f"   èžåˆå‘¨æœŸ: {', '.join(timeframes)}")
    log(f"   æ•°æ®æ¨¡å¼: {source_mode.upper()}")
    log(f"   è¾“å‡ºæ ¼å¼: {output_fmt.upper()}")
    if variant in ("fixed", "roll", "both"):
        log(f"   è¾“å‡ºå˜ä½“(mergeæº): {variant.upper()}")
    log(f"\nðŸ“‚ è·¯å¾„é…ç½®:")
    log(f"   åŸºç¡€Kçº¿: {base_filename}")
    log(f"   æŒ‡æ ‡ç›®å½•: {ind_dir}")
    log(f"   è¾“å‡ºæ–‡ä»¶: {output_filename}")
    log(f"\nðŸ”§ èžåˆé€‰é¡¹:")
    log(f"   å¯¹é½æ–¹å¼: {align_direction}")
    log(f"   åŒ…å«OHLCV: {include_base_ohlcv}")
    log(f"   å‘¨æœŸå‰ç¼€: {add_prefix}")
    if start or end:
        log(f"\nâ° æ—¶é—´èŒƒå›´: {start or '-âˆž'} ~ {end or '+âˆž'}")

    if include_base_ohlcv:
        log(f"\nâœ“ è¯»å–åŸºç¡€Kçº¿...")
        try:
            if str(base_axis_tf).lower() == str(base_download).lower():
                df0 = io.read_table("download", timeframe=base_download)
            else:
                df0 = io.read_table("kline", timeframe=base_axis_tf)
            if "timestamp" in df0.columns:
                df0["timestamp"] = pd.to_datetime(df0["timestamp"], errors="coerce")
                df0 = df0.set_index("timestamp")
            df0.index.name = "timestamp"
            keep_cols = [c for c in ["open", "high", "low", "close", "volume"] if c in df0.columns]
            base_df = df0[keep_cols].sort_index()
        except Exception as e:
            raise RuntimeError(f"æœªæ‰¾åˆ°ä¸»è½´Kçº¿({base_axis_tf})ï¼Œè¯·å…ˆè¿è¡ŒStep2ç”Ÿæˆã€‚è¯¦æƒ…: {e}") from e
        log(f"  åŸºç¡€æ•°æ®: {len(base_df):,} è¡Œ")
        log(f"  æ—¶é—´èŒƒå›´: {base_df.index.min()} ~ {base_df.index.max()}")
        log(f"  OHLCVåˆ—: {list(base_df.columns)}")
    else:
        base_df = None
        log(f"\nâ­ï¸ è·³è¿‡åŸºç¡€Kçº¿ï¼ˆä»…èžåˆæŒ‡æ ‡ï¼‰")

    log(f"\nâœ“ è¯»å–å¤šå‘¨æœŸæŒ‡æ ‡...")
    tf_to_df: dict[str, pd.DataFrame] = {}
    for i, tf in enumerate(timeframes, 1):
        log(f"  [{i}/{len(timeframes)}] å‘¨æœŸ {tf}...", end=" ")
        try:
            df0 = io.read_table("indicator", timeframe=tf)
            if "timestamp" in df0.columns:
                df0["timestamp"] = pd.to_datetime(df0["timestamp"], errors="coerce")
                df0 = df0.set_index("timestamp")
            df0.index.name = "timestamp"
            df0 = df0[~df0.index.duplicated(keep="last")].sort_index()
            if start:
                df0 = df0[df0.index >= pd.to_datetime(start)]
            if end:
                df0 = df0[df0.index <= pd.to_datetime(end)]
            ohlcv_cols = ["open", "high", "low", "close", "volume"]
            df0 = df0.drop(columns=[c for c in ohlcv_cols if c in df0.columns], errors="ignore")
            df0 = _standardize_indicator_columns(df0)
            tf_to_df[tf] = df0
            log(f"âœ“ {len(df0):,} è¡Œ, {df0.shape[1]} åˆ—ç‰¹å¾")
        except FileNotFoundError as e:
            log(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            if verbose:
                log(f"      {e}")
        except Exception as e:
            log(f"âŒ è¯»å–å¤±è´¥: {e}")

    if not tf_to_df:
        raise RuntimeError("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•å‘¨æœŸçš„æŒ‡æ ‡æ•°æ®")

    log(f"\nâœ“ å¼€å§‹èžåˆ...")
    if base_df is None:
        raise RuntimeError("å½“å‰é…ç½®æœªåŒ…å«åŸºç¡€Kçº¿ï¼Œç¦»çº¿èžåˆå»ºè®®å¯ç”¨ include_base_ohlcv=True")

    merged = base_df.reset_index().sort_values("timestamp").copy()
    log(f"  ä¸»è½´: {len(merged):,} è¡Œ")

    for tf, df in tf_to_df.items():
        if add_prefix:
            df_prefixed = prefix_columns(df, tf)
        else:
            df_prefixed = df.copy()
        df_prefixed = df_prefixed.reset_index().sort_values("timestamp")
        merged = pd.merge_asof(
            merged,
            df_prefixed,
            on="timestamp",
            direction=align_direction,
            allow_exact_matches=allow_exact_match,
        )
        log(f"  + {tf}: {df_prefixed.shape[1]-1} åˆ—ç‰¹å¾ â†’ ç´¯è®¡ {merged.shape[1]-1} åˆ—")

    merged = merged.set_index("timestamp").sort_index()

    if add_prefix and include_base_ohlcv:
        base_tf = str(base_axis_tf)
        log(f"\nâœ“ ä¸ºåŸºç¡€Kçº¿æ·»åŠ å‘¨æœŸå‰ç¼€: {base_tf}_*")
        base_prefix_map = {
            "open": f"{base_tf}_open",
            "high": f"{base_tf}_high",
            "low": f"{base_tf}_low",
            "close": f"{base_tf}_close",
            "volume": f"{base_tf}_volume",
        }
        cols_to_rename = {c: base_prefix_map[c] for c in base_prefix_map.keys() if c in merged.columns}
        if cols_to_rename:
            merged = merged.rename(columns=cols_to_rename)

    if merge_cfg.get("enable_real_sliding", True):
        log(f"\nâœ“ åº”ç”¨çœŸæ»‘çª—è®¡ç®—...")
        try:
            from features_engineering.tools.real_sliding_simple import apply_real_sliding_window

            merged = apply_real_sliding_window(merged, timeframes, base_axis_tf, merge_cfg)
        except Exception as e:
            log(f"   âš ï¸ çœŸæ»‘çª—è®¡ç®—å¤±è´¥ï¼ˆå·²å¿½ç•¥ï¼‰: {e}")
            if verbose:
                import traceback

                traceback.print_exc()

    log(f"\n{'='*70}")
    log("èžåˆå®Œæˆç»Ÿè®¡")
    log(f"{'='*70}")
    log(f"æœ€ç»ˆæ•°æ®: {len(merged):,} è¡Œ Ã— {merged.shape[1]} åˆ—")
    log(f"æ—¶é—´èŒƒå›´: {merged.index.min()} ~ {merged.index.max()}")
    log(f"ç‰¹å¾åˆ—æ•°: {merged.shape[1]}")

    if add_prefix:
        log(f"\nåˆ—åˆ†å¸ƒ:")
        for tf in timeframes:
            tf_cols = [c for c in merged.columns if c.startswith(f"{tf}_")]
            if tf_cols:
                log(f"  {tf}: {len(tf_cols)} åˆ—")
        if include_base_ohlcv:
            base_tf = str(base_axis_tf)
            base_cols = [c for c in merged.columns if c.startswith(f"{base_tf}_")]
            if base_cols:
                log(f"  {base_tf} (åŸºç¡€): {len(base_cols)} åˆ—")

    log(f"\nâœ“ ä¿å­˜èžåˆç»“æžœ...")
    if not io_overwrite:
        try:
            base, ext = os.path.splitext(output_file)
            for cand in (base + ".parquet", base + ".csv"):
                if os.path.exists(cand):
                    old = read_df_auto(cand)
                    if "timestamp" in old.columns:
                        old["timestamp"] = pd.to_datetime(old["timestamp"], errors="coerce")
                        old = old.set_index("timestamp")
                    old.index.name = "timestamp"
                    merged = safe_concat_dedup(old, merged)
                    log(f"   ðŸ” åˆå¹¶åŽ†å² merged: {len(merged):,} è¡Œ")
                    break
        except Exception as _e:
            log(f"   âš ï¸ åˆå¹¶åŽ†å²å¤±è´¥(merged): {_e}")

    save_output(merged, output_file, output_fmt)

    log(f"\n{'='*70}")
    log("ðŸŽ‰ ç‰¹å¾èžåˆå®Œæˆï¼")
    log(f"{'='*70}\n")

    if verbose:
        print_latest_timestamp_from_df(merged)

    return {
        "symbol": symbol,
        "market_type": market_type,
        "base_download": base_download,
        "base_axis": base_axis_tf,
        "timeframes": timeframes,
        "kline_dir": kline_dir,
        "indicators_dir": ind_dir,
        "merged_dir": merged_dir,
        "output_file": output_file,
        "output_format": output_fmt,
        "include_base_ohlcv": include_base_ohlcv,
        "align_direction": align_direction,
        "add_prefix": add_prefix,
        "start": start,
        "end": end,
    }


def main():
    """ä¸»æµç¨‹ï¼šå®Œå…¨é…ç½®é©±åŠ¨çš„ç‰¹å¾èžåˆ"""
    args = parse_args()

    try:
        loader = ConfigLoader()
        cfg = loader.load_step4_config()
        if not cfg:
            print("âŒ æœªæ‰¾åˆ°æˆ–æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶")
            return 1

    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return 1
    try:
        execute_step4(
            cfg,
            start=args.start,
            end=args.end,
            output_format=args.output_format,
            verbose=True,
        )
        return 0
    except Exception as e:
        print(f"âŒ Step4 æ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
